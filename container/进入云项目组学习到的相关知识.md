# 云项目组“上天”过程中发现的那些“神仙”

来云组已经两个月有余了，在我们组上天的过程中，碰到了很多之前开发中完全用不到的技术知识点，也就是我所说的“神仙”，今天我就来给大家盘一盘这些个“神仙”到底都是些什么玩意。

## 继续从PaaS说起

### 容器的兴起

强哥在之前的某次sprint技术分享会中给大家讲过了PaaS和SaaS等概念，那我这次就继续从PaaS开始，先来讲讲他的前世今生吧。

早在2013年前后，服务端的技术是云端计算的天下，当时有大量的云计算厂商涌入市场，比较著名的有`AWS`和`OpenStack`，他们将虚无缥缈的云端计算这种高大上的概念，固化成了实实在在的虚拟机技术，当时的主流用户，也都是将自己的应用上传到云端的虚拟机中，以之前管理物理机的方式管理着可能是一群云端的虚拟服务器。

而就在这个时候，一家主打开源PaaS项目的平台`Cloud Foundry`诞生了。与传统的提供云上虚拟机服务不同，Cloud Foundry的PaaS所提供的服务是`应用托管`的功能，也就是用户只需要通过一条简单的命令比如：`cf push "我的应用"`，就可以把自己的项目部署在可能成千上万台的终端服务器上。然后平台对这些项目提供分发，灾备，监控，重启等等服务（这其实也是我们组想要给用户提供的最终服务），这些托管服务解放了开发者的生产力，让他们不用再关心应用的运维状况，而是专心开发自己的应用。这就是PaaS的概念，平台即服务。

因此，像Cloud Foundry这样的PaaS项目，最核心的组件就是对一套应用的打包和分发机制，Cloud Foundry为每一种主流的语言都定义了一套打包的方式。而cf push命令，就是将应用打好的包上传到云端的存储中，然后Cloud Foundry调度出一个可运行的虚拟机，通知该虚拟机上的Agent下载并且运行这个打包结果。由此可见，Cloud Foundry的虚拟机中会运行多个用户的多个服务，为了解决服务之间相互隔离的边界问题，Cloud Foundry使用了Linux的Namespace技术对运行的应用进行了隔离和分组，使用Cgroup技术对隔离的应用分配资源，而这也正是容器技术的一部分。

由此我们可以知道，容器技术并不是Docker创建的，而且在Docker兴起之前，就已经被其他公司商用了，但是为什么现在一谈起容器，所有人第一时间想到的就是Docker呢？

上面，我提到了Cloud Foundry的部署需要用户对其应用进行打包，然而就是这个打包的功能，成了Cloud Foundry的一个软肋一直被用户诟病：为了上传PaaS，用户就不得不为每一种语言，每一种框架，甚至是每个版本应用维护一个打好的包，这种打包的方法是毫无章法的，还有可能出现本机运行成功，打了个包上传上去之后就无法运行的情况。然而就在这个时候，这一节的主人`Docker`开始崭露头角了。

Docker是一个当时还叫dotCloud的公司开发的容器项目，在开源的短短几个月后就迅速崛起，并一局将Cloud Foundry赶出了局，然而最可笑的是，在Docker刚开源的时候，Cloud Foundry的首席产品经理 James Bayer就在社区做了一次详细的对比，告诉用户Docker和Cloud Foundry一样是一个使用了Namespace和Cgroup技术的沙箱而已，没什么值得关注的。事实上，Docker也确实就和他所说的一样，但是只做了一点小小的创新，而就是这一点小小的创新，对Cloud Foundry造成了毁灭性的降维打击。这个创新就是`Docker镜像`。

Docker镜像几乎完美地解决了Cloud Foundry对于打包方面的软肋。所谓的镜像，其实也是一个压缩包，但是比起Cloud Foundry那种执行文件+启动脚本的打包结果，镜像提供给用户的是一套完整的运行环境，每一个镜像都可以指定操作系统版本，内部可以构建程序执行的文件结构，并且一份镜像可以完全共享多处使用。Docker给用户提供了一套完善的镜像制作流程，用户也就不用关心自己的语言和框架了，只需要定制对应程序所需要的运行的操作系统环境即可（这一点其实也可以简单的看成是对压缩包这种应用做了底层的抽象，比较契合软件设计的原则。也可以看到，软件行业的一点创新，带来的就是降维级别的打击）。总结一下就是：`Docker 镜像完美解决了两个问题：1是本地环境和服务器环境的差异问题；2是一份镜像所有的机器都可以运行的复用问题`。

在这之后，PaaS的市场，已经完全是Docker的天下了。而之后的故事，我会在下一节再继续讲。

### 一个进程的诞生

好了，讲了半天的故事了，也要说一点干货了：容器到底是个什么玩意，能有这么大的市场和这么多服务端程序员对其青睐？先卖个关子，我先从一个进程说起。

我们都知道，计算机里运行的程序其实都是一个一个的进程，而一个进程其实就是程序执行之后，从磁盘的二进制文件，到内存、寄存器、堆栈指令等等所用到的相关设备状态的一个集合，是`数据和状态综合的动态表现`。而容器技术，其实就是对一个进程的状态和数据进行的一系列`隔离`和`限制`后的结果。因此可以知道，`容器的本质其实就是Linux中的一个特殊进程`。

先来说说隔离，我们可以在任意一个装有Docker程序的Linux中执行下列的命令创建一个简单的镜像：

``` shell
$ docker run -it busybox /bin/sh
```

这条语句的大概意思是：用docker运行一个容器，容器的镜像名称叫`busybox`，并且运行之后需要执行的命令是`/bin/sh`，而`-it`参数表示需要使用标准输入`stdin`和分配一个文本输入输出环境`tty`与外部交互。通过这个命令，我们就可以进入到一个容器内部了，分别在容器中和宿主机中执行`top`命令，可以看到以下结果：

![host_container_comparer](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/host_container_comparer.png)

可以发现，容器中的运行进程只剩下了两个，一个就是主进程PID==1的/bin/sh，另一个就是我们运行的top，而宿主机中的其余的所有进程在容器中都看不到了，这就是`隔离`。本来，每当我们在宿主机上运行一个/bin/sh程序，操作系统都会给它分配一个进程编号，比如PID==100。而现在，我们要通过Docker把这个/bin/sh程序运行在一个容器中，这时候，Docker就会在这个PID==100创建时施加一个“障眼法”，让他永远看不到之前的99个进程，这样运行在容器中的程序就会当自己是PID==1的主进程。

> 这里说明一下，在Linux操作系统中，PID==1的进程被称为超级进程，它是整个进程树的root，负责产生其他所有用户进程。所有的进程都会被挂在这个进程下，如果这个进程退出了，那么所有的进程都被 kill

而这种机制，其实就是对被隔离的程序的进程空间做了手脚，虽然在容器中显示的PID==1，但是在原本的宿主机中，它其实还是那个PID==100的进程。所使用到的技术就是Linux中的`Namespace机制`。而这个机制，其实就是Linux在创建进程时的一个可选参数。在Linux中，创建一个线程的函数是（这里没写错就是线程，Linux中线程是用进程实现的，所以可以用来描述进程）：

``` shell
int pid = clone(main_function, stack_size, SIGCHLD, NULL);
```

如果我们给这个方法添加一个参数比如`CLONE_NEWPID`：

``` shell
int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL);
```

那么这个新的进程就会看到一个全新的进程空间，在这个空间里，它自己的PID就等于1。

`除了刚才说的PID Namespace，Linux操作系统还提供了Mount、UTS、IPC、Network和User的Namespace，用来对各种不同的进程上下文进行隔离操作`。

以上，也就是Linux容器最基本的隔离的实现了。

说完了隔离，再来说说限制。容器的隔离性是通过Linux的`Cgroup`实现的。`Cgroup`的全称是`Linux Control Group`，是Linux操作系统中用来`限制一个进程组使用资源的上限，包括CPU、内存、磁盘、网络带宽等`的功能。在Linux中，Cgroup给用户暴露的API是文件系统，因此用户可以通过修改文件的值来操作Cgroup功能。在Linux系统（Ubuntu）中可以执行以下命令查看CgroupAPI文件：

``` shell
mount -t cgroup
```

![Cgroup_fs](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cgroup_fs.png)

从上图可以看到，系统中存在包括cpu、内存、IO等多个Cgroup配置文件。我们可以以CPU为例来说明以下Cgroup这个功能。对CPU的限制需要引入两个参数`cfs_period`和`cfs_quota`，这个强哥肯定知道当时为了给Docker内的程序限制CPU会经常操作这两个参数，这两个参数是组合使用的，意思是在长度为cfs_period时间内，程序组只能分到总量为cfs_quota的CPU时间。也就是说`cfs_quota / cfs_period == cpu使用上限`。

举个栗子，在`/sys/fs/cgroup/cpu`目录下，执行以下命令创建一个文件夹container：

``` shell
/sys/fs/cgroup/cpu/ > mkdir container
```

可以发现系统会自动为container目录下生成一系列的CPU限制的参数文件，这是Linux系统自动生成的，表示我们成功为CPU创建了一个控制组container：

![cpu_limit_file](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_limit_file.png)

然后我们可以执行一下以下脚本创建一个死循环程序：

``` shell
while : ; do : ; done &
```

会看到返回的`进程为398`，因为死循环所以top可以看到cpu占用率为100%：

![cpu_full](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_full.png)

这时，我们可以看下container目录下的`cpu.cfs_quota_us`和`cpu.cfs_period_us`：

![cpu_usage_file](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_usage_file.png)

`cfs_quota_us`为-1说明并没有限制CPU的运行上限，现在我们可以改一下这个值：

``` shell
echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
```

然后将之前的`进程398`写入这个控制组的`tasks`文件中：

``` shell
echo 398 > /sys/fs/cgroup/cpu/container/tasks
```

这时，我们可以再top一下，发现刚才的死循环的CPU使用率只有20%了：

![cpu_usage_after](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_usage_after.png)

以上，就是通过Cgroup功能对容器做限制的原理了，同理可以用此方法，对一个容器的内存、带宽等做限制，这样，一个简单的容器基本就可以展现在你面前了，Cloud Foundry为自己的PaaS平台为每一个应用所创建的就是这样的一个容器。

> 这里要做一个特别的说明，只有Linux中运行的容器是通过对进程进行限制模拟出来的结果，Windows和Mac下的容器，都是通过Docker Desktop操作虚拟机模拟出来的真实的虚拟容器。

### Docker做了些什么？

从第一节我们知道，Docker通过自己的`Docker 镜像`功能迅速取代了Cloud Foundry，那这个Docker镜像到底是什么呢？先卖个关子，我们先来看看之前说过的隔离功能中的另一个机制`Namespace机制`。

`Mount Namespace`从名字你们也知道这个是和文件挂载相关的，是用来隔离进程的挂载目录的，我们可以通过一个“简单”的栗子来看看它是怎么工作的：

``` C
#define _GNU_SOURCE
#include <sys/mount.h> 
#include <sys/types.h>
#include <sys/wait.h>
#include <stdio.h>
#include <sched.h>
#include <signal.h>
#include <unistd.h>
#define STACK_SIZE (1024 * 1024)
static char container_stack[STACK_SIZE];
char* const container_args[] = {
    "/bin/bash",
    NULL
};

int container_main(void* arg)
{  
    printf("Container - inside the container!\n");
    execv(container_args[0], container_args);
    printf("Something's wrong!\n");
    return 1;
}

int main()
{
    printf("Parent - start a container!\n");
    int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL);
    waitpid(container_pid, NULL, 0);
    printf("Parent - container stopped!\n");
    return 0;
}
```

上面是一个“简单的”的C语言代码，如果刨去现在已经不记得怎么写C语言了之外，这个代码其实真的挺简单的，只有两个逻辑：1是在main函数中创建了一个子进程，并且传递了一个参数`CLONE_NEWNS`，这个参数就是用来实现`Mount Namespace`的；2是在子进程中调用了`/bin/bash`命令运行了一个子进程内部的shell。让我们编译并且执行一下这个程序：

``` shell
gcc -o ns ns.c
./ns
```

这样我们就进入了这个子进程的shell中，我们可以运行`ls /tmp`和宿主机进行一下对比：

![mount_namespace_show_demo](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/mount_namespace_show_demo.png)

你会发现为啥两边展示的数据会完全一样？因为按照上一部分Cpu Namespace的结论，应该分别看到两个不同的文件目录才对。这是因为`Mount Namespace`修改的，是进程对文件系统“挂载点”的认知，意思也就是只有发生了`挂载`这个操作之后生成的所有目录才会是一个新的系统，而如果不做挂载操作，那就和宿主机的完全一致。那如何解决这个问题呢？其实就是在创建进程时，除了声明一个`Mount Namespace`之外还告诉这个进程需要进行一次挂载操作就可以了，可以简单修改一下新进程的代码，然后运行查看：

``` C
int container_main(void* arg)
{  
    printf("Container - inside the container!\n");
    mount("none", "/tmp", "tmpfs", 0, "");
    execv(container_args[0], container_args);
    printf("Something's wrong!\n");
    return 1;
}
```

![mount_namespace_show_demo_new](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/mount_namespace_show_demo_new.png)

现在就会发现，对文件的隔离已经生效了，并且在子进程内部新创建的目录宿主机中一样看不到了，因为子进程的`/tmp`已经被挂载进了`tmpfs（一个内存盘）`中了，这就相当于创建了完全一个新的`tmp`环境。

为什么要花这么大的篇幅来说这个呢，是因为它就`是Docker镜像的主要技术之一`。上面的栗子，我只是把`tmp`这个目录进行了挂载，同样的，在Cloud Foundry中挂载隔离的文件也都是应用所需要的一些目录，而Docker镜像做的，是把`整个根目录`全部挂载进了这个新的进程中。

> 这里要稍微介绍一下Linux系统地文件结构的知识：和我们平时使用的Windows操作系统不同，Linux操作系统的系统**内核**与**操作系统所包含的文件、配置和目录**是`分开存放的`，系统只有在开机启动时才会加载指定版本的内核镜像。Linux所使用到的文件配置目录被称为`根文件系统（rootfs）`，也就是我们日常操作Linux看到的`/`目录。

Docker镜像的本质，其实也就是对rootfs的一次封装，Docker将一个应用所需操作系统的rootfs，通过`Mount Namespace`进行了封装，这样带来的直接结果就是：**改变了应用程序和操作系统的依赖关系**（这个很类似于强哥上一次讲到的控制反转的理念）即原本应用程序是在操作系统内运行的，而Docker把“操作系统”封装变成了应用程序的依赖库，这样就解决了应用程序运行环境`一致性`的问题，因为不管你是在自己主机上，还是服务器上，又或者是其他什么机器上，因为应用所运行的系统已经成了一个“依赖库”了，所以每个地方运行，这个一致性是完全可以保证的。

但是目前只解决了一致性，复用性的问题还没有解决，即我不可能每制作一个镜像就挂载一个新的rootfs吧，这样先不说复用性了，光硬盘都需要占用很大的空间来实现这些挂载内容。因此，Docker还为镜像使用了`另一个主要技术：UnionFS`以及一个`全新的概念：层（layer）`。

先说一下`UnionFS`是干什么的，UnionFS是一个联合挂载的功能，它可以将多个路径下的文件联合挂载到同一个目录下，举个栗子，现在有一个如下的目录结构：

``` shell
$ tree
.
├── A
│   ├── a
│   └── x
│ 
└── B
    ├── b
    └── x
```

A目录下有a和x两个文件，B目录下有b和x两个文件，通过UnionFS的功能，我们可以将这两个目录挂载到C目录下：

``` shell
$ tree ./C
C
├── a
├── b
└── x
```

最终C目录下的x只有一份，并且如果你对C目录下的a、b、x修改，之前目录A和B中的文件同样会被修改。而Docker正是用了这个技术，对其镜像进行了联合挂载，比如可以分别把`/sys`，`/etc`,`/tmp`目录一起挂载到rootfs中形成一个在子进程看起来就是一个完整的rootfs。

此外，Docker还自己创新了一个层的概念，它将系统内核所需要的rootfs内的文件挂载到了一个`只读层`中，将用户的应用程序、系统的配置文件等之类可以修改的文件挂载到了`可读写层`中，并且在容器启动时可能会用到的初始化参数挂载到了`init层`中。之后，将这三层再次联合挂载，最终形成了容器中的rootfs。这时你应该明白了，这个`只读层`是几乎同样版本的代码几乎不会改变的，比如我们的.net core项目用到的基础环境等等都会设计在了只读层，每次获取最新镜像时，因为每一份只读层都是完全一样的，所以完全不用下载，这也解释了为什么Docker镜像只在第一次下载时那么慢，而之后的镜像都很快，并且明明每份镜像看起来都几百兆，但是最终机器上的硬盘缺没有占用那么多的原因。也是这个技术，解决了镜像复用性的问题。我们可以看一下Docker镜像分层的图：

![docker_layer](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/docker_layer.png)

这里有个问题不知道你有没有考虑过，如果我们的程序做的是删除只读层中的文件，那会发生什么事呢？其实解决方案是：Docker在可读写层加入了一个whiteout的文件夹（BLM政治不正确），所以在只读层做的操作其实就在这个文件夹中加入了一个新的文件，然后在容器中就通过这个文件做了`“逻辑删除”`，只是做了`“遮挡”`的操作，原本的文件还是存在的。

好了，以上就是Docker容器的整个原理了，我们可以总结一下，Docker创建容器的过程其实是：

1. 启用Linux Namespace配置；
2. 设置指定的Cgroup参数；
3. 切换进程的根目录

对于制作一个Docker镜像，Docker使用了联合挂载的功能，和创建了层的概念，解决了镜像的一致性和复用性。

# 参考文档

* 极客时间：[深入剖析Kubernetes](https://time.geekbang.org/column/intro/100015201)、[Elasticsearch核心技术与实践](https://time.geekbang.org/course/intro/100030501)
* CoolShell：[Linux PID 1 和Systemd](https://coolshell.cn/articles/17998.html)、[DOCKER基础技术：LINUX NAMESPACE（上）](https://coolshell.cn/articles/17010.html)
* Wikipedia：[文件系统层次结构标准](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard)