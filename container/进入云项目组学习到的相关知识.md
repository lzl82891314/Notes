# 云项目组“上天”过程中发现的那些“神仙”

来云组已经两个月有余了，在我们组上天的过程中，碰到了很多之前开发中完全用不到的技术知识点，也就是我所说的“神仙”，今天我就来给大家盘一盘这些个“神仙”到底都是些什么玩意。

## 继续从PaaS说起

### 容器的兴起

强哥在之前的某次sprint技术分享会中给大家讲过了PaaS和SaaS等概念，那我这次就继续从PaaS开始，先来讲讲他的前世今生吧。

早在2013年前后，服务端的技术是云端计算的天下，当时有大量的云计算厂商涌入市场，比较著名的有`AWS`和`OpenStack`，他们将虚无缥缈的云端计算这种高大上的概念，固化成了实实在在的虚拟机技术，当时的主流用户，也都是将自己的应用上传到云端的虚拟机中，以之前管理物理机的方式管理着可能是一群云端的虚拟服务器。

而就在这个时候，一家主打开源PaaS项目的平台`Cloud Foundry`诞生了。与传统的提供云上虚拟机服务不同，Cloud Foundry的PaaS所提供的服务是`应用托管`的功能，也就是用户只需要通过一条简单的命令比如：`cf push "我的应用"`，就可以把自己的项目部署在可能成千上万台的终端服务器上。然后平台对这些项目提供分发，灾备，监控，重启等等服务（这其实也是我们组想要给用户提供的最终服务），这些托管服务解放了开发者的生产力，让他们不用再关心应用的运维状况，而是专心开发自己的应用。这就是PaaS的概念，平台即服务。

因此，像Cloud Foundry这样的PaaS项目，最核心的组件就是对一套应用的打包和分发机制，Cloud Foundry为每一种主流的语言都定义了一套打包的方式。而cf push命令，就是将应用打好的包上传到云端的存储中，然后Cloud Foundry调度出一个可运行的虚拟机，通知该虚拟机上的Agent下载并且运行这个打包结果。由此可见，Cloud Foundry的虚拟机中会运行多个用户的多个服务，为了解决服务之间相互隔离的边界问题，Cloud Foundry使用了Linux的Namespace技术对运行的应用进行了隔离和分组，使用Cgroups技术对隔离的应用分配资源，而这也正是容器技术的一部分。

由此我们可以知道，容器技术并不是Docker创建的，而且在Docker兴起之前，就已经被其他公司商用了，但是为什么现在一谈起容器，所有人第一时间想到的就是Docker呢？

上面，我提到了Cloud Foundry的部署需要用户对其应用进行打包，然而就是这个打包的功能，成了Cloud Foundry的一个软肋一直被用户诟病：为了上传PaaS，用户就不得不为每一种语言，每一种框架，甚至是每个版本应用维护一个打好的包，这种打包的方法是毫无章法的，还有可能出现本机运行成功，打了个包上传上去之后就无法运行的情况。然而就在这个时候，一个开源项目在社区的热度开始逐渐升温了，这个项目就是第一节的主人公`Docker`。

Docker是一个当时还叫dotCloud的公司开发的容器项目，在开源的短短几个月后就迅速崛起，并一举将Cloud Foundry赶出了局，然而最可笑的是，在Docker刚开源的时候，Cloud Foundry的首席产品经理 James Bayer就在社区做了一次详细的对比，告诉用户Docker和Cloud Foundry一样是一个使用了Namespace和Cgroups技术的沙箱而已，没什么值得关注的。事实上，Docker也确实就和他所说的一样，但是只做了一点小小的创新，而就是这一点小小的创新，对Cloud Foundry造成了毁灭性的打击，这个创新就是`Docker镜像`。

Docker镜像几乎完美地解决了Cloud Foundry对于打包方面的软肋。所谓的镜像，其实也是一个压缩包，但是比起Cloud Foundry那种执行文件+启动脚本的打包结果，镜像提供给用户的是一套完整的运行环境，每一个镜像都可以指定操作系统版本，内部可以构建程序执行的文件结构，并且一份镜像可以完全共享多处使用。Docker给用户提供了一套完善的镜像制作流程，用户也就不用关心自己的语言和框架了，只需要定制对应程序所需要的运行的操作系统环境即可（这一点其实也可以简单的看成是对压缩包这种应用做了底层的抽象，比较契合软件设计的原则。也可以看到，软件行业的一点创新，带来的就是降维级别的打击）。总结一下就是：`Docker 镜像完美解决了两个问题：1是本地环境和服务器环境的差异问题；2是一份镜像所有的机器都可以运行的复用问题`。

在这之后，PaaS的市场，已经完全是Docker的天下了。而之后的故事，我会在下一节再继续讲。

### 一个进程的诞生

好了，讲了半天的故事了，也要说一点干货了：容器到底是个什么玩意，能有这么大的市场和这么多服务端程序员对其青睐？先别着急，我先从一个进程说起。

我们都知道，计算机里运行的程序其实都是一个一个的进程，而一个进程其实就是程序执行之后，从磁盘的二进制文件，到内存、寄存器、堆栈指令等等所用到的相关设备状态的一个集合，是`数据和状态综合的动态表现`。而容器技术，其实就是对一个进程的状态和数据进行的一系列`隔离`和`限制`后的结果。因此可以知道，`容器的本质其实就是Linux中的一个特殊进程`。

先来说说隔离，我们可以在任意一个装有Docker程序的Linux中执行下列的命令创建一个简单的镜像：

``` shell
$ docker run -it busybox /bin/sh
```

这条语句的大概意思是：用docker运行一个容器，容器的镜像名称叫`busybox`，并且运行之后需要执行的命令是`/bin/sh`，而`-it`参数表示需要使用标准输入`stdin`和分配一个文本输入输出环境`tty`与外部交互。通过这个命令，我们就可以进入到一个容器内部了，分别在容器中和宿主机中执行`top`命令，可以看到以下结果：

![host_container_comparer](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/host_container_comparer.png)

可以发现，容器中的运行进程只剩下了两个，一个就是主进程PID==1的/bin/sh，另一个就是我们运行的top，而宿主机中的其余的所有进程在容器中都看不到了，这就是`隔离`。本来，每当我们在宿主机上运行一个/bin/sh程序，操作系统都会给它分配一个进程编号，比如PID==100。而现在，我们要通过Docker把这个/bin/sh程序运行在一个容器中，这时候，Docker就会在这个PID==100创建时施加一个“障眼法”，让他永远看不到之前的99个进程，这样运行在容器中的程序就会当自己是PID==1的主进程。

> 这里说明一下，在Linux操作系统中，PID==1的进程被称为超级进程，它是整个进程树的root，负责产生其他所有用户进程。所有的进程都会被挂在这个进程下，如果这个进程退出了，那么所有的进程都被 kill

而这种机制，其实就是对被隔离的程序的进程空间做了手脚，虽然在容器中显示的PID==1，但是在原本的宿主机中，它其实还是那个PID==100的进程。所使用到的技术就是Linux中的`Namespace机制`。而这个机制，其实就是Linux在创建进程时的一个可选参数。在Linux中，创建一个线程的函数是（这里没写错就是线程，Linux中线程是用进程实现的，所以可以用来描述进程）：

``` shell
int pid = clone(main_function, stack_size, SIGCHLD, NULL);
```

如果我们给这个方法添加一个参数比如`CLONE_NEWPID`：

``` shell
int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL);
```

那么这个新的进程就会看到一个全新的进程空间，在这个空间里，它自己的PID就等于1。

`除了刚才说的PID Namespace，Linux操作系统还提供了Mount、UTS、IPC、Network和User的Namespace，用来对各种不同的进程上下文进行隔离操作`。

以上，也就是Linux容器最基本的隔离的实现了。

说完了隔离，再来说说限制。容器的隔离性是通过Linux的`Cgroups`实现的。`Cgroups`的全称是`Linux Control Group`，是Linux操作系统中用来`限制一个进程组使用资源的上限，包括CPU、内存、磁盘、网络带宽等`的功能。在Linux中，Cgroups给用户暴露的API是文件系统，因此用户可以通过修改文件的值来操作Cgroups功能。在Linux系统（Ubuntu）中可以执行以下命令查看CgroupsAPI文件：

``` shell
mount -t Cgroups
```

![cgroups_file](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cgroups_file.png)

从上图可以看到，系统中存在包括cpu、内存、IO等多个Cgroups配置文件。我们可以以CPU为例来说明以下Cgroups这个功能。对CPU的限制需要引入两个参数`cfs_period`和`cfs_quota`，这个强哥肯定知道当时为了给Docker内的程序限制CPU会经常操作这两个参数，这两个参数是组合使用的，意思是在长度为cfs_period时间内，程序组只能分到总量为cfs_quota的CPU时间。也就是说`cfs_quota / cfs_period == cpu使用上限`。

举个栗子，在`/sys/fs/Cgroups/cpu`目录下，执行以下命令创建一个文件夹container：

``` shell
/sys/fs/Cgroups/cpu/ > mkdir container
```

可以发现系统会自动为container目录下生成一系列的CPU限制的参数文件，这是Linux系统自动生成的，表示我们成功为CPU创建了一个控制组container：

![cpu_limit_file](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_limit_file.png)

然后我们可以执行一下以下脚本创建一个死循环程序：

``` shell
while : ; do : ; done &
```

会看到返回的`进程为398`，因为死循环所以top可以看到cpu占用率为100%：

![cpu_full](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_full.png)

这时，我们可以看下container目录下的`cpu.cfs_quota_us`和`cpu.cfs_period_us`：

![cpu_usage_file](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_usage_file.png)

`cfs_quota_us`为-1说明并没有限制CPU的运行上限，现在我们可以改一下这个值：

``` shell
echo 20000 > /sys/fs/Cgroups/cpu/container/cpu.cfs_quota_us
```

然后将之前的`进程398`写入这个控制组的`tasks`文件中：

``` shell
echo 398 > /sys/fs/Cgroups/cpu/container/tasks
```

这时，我们可以再top一下，发现刚才的死循环的CPU使用率只有20%了：

![cpu_usage_after](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_usage_after.png)

以上，就是通过Cgroups功能对容器做限制的原理了，同理可以用此方法，对一个容器的内存、带宽等做限制，这样，一个简单的容器基本就可以展现在你面前了，Cloud Foundry为自己的PaaS平台为每一个应用所创建的就是这样的一个容器。

> 这里要做一个特别的说明，只有Linux中运行的容器是通过对进程进行限制模拟出来的结果，Windows和Mac下的容器，都是通过Docker Desktop操作虚拟机模拟出来的真实的虚拟容器。

### Docker做了些什么？

从第一节我们知道，Docker通过自己的`Docker 镜像`功能迅速取代了Cloud Foundry，那这个Docker镜像到底是什么呢？先卖个关子，我们先来看看之前说过的隔离功能中的另一个机制`Namespace机制`。

`Mount Namespace`从名字你们也知道这个是和文件挂载相关的，是用来隔离进程的挂载目录的，我们可以通过一个“简单”的栗子来看看它是怎么工作的：

``` C
#define _GNU_SOURCE
#include <sys/mount.h> 
#include <sys/types.h>
#include <sys/wait.h>
#include <stdio.h>
#include <sched.h>
#include <signal.h>
#include <unistd.h>
#define STACK_SIZE (1024 * 1024)
static char container_stack[STACK_SIZE];
char* const container_args[] = {
    "/bin/bash",
    NULL
};

int container_main(void* arg)
{  
    printf("Container - inside the container!\n");
    execv(container_args[0], container_args);
    printf("Something's wrong!\n");
    return 1;
}

int main()
{
    printf("Parent - start a container!\n");
    int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL);
    waitpid(container_pid, NULL, 0);
    printf("Parent - container stopped!\n");
    return 0;
}
```

上面是一个“简单的”的C语言代码，如果刨去现在已经不记得怎么写C语言了之外，这个代码其实真的挺简单的，只有两个逻辑：1是在main函数中创建了一个子进程，并且传递了一个参数`CLONE_NEWNS`，这个参数就是用来实现`Mount Namespace`的；2是在子进程中调用了`/bin/bash`命令运行了一个子进程内部的shell。让我们编译并且执行一下这个程序：

``` shell
gcc -o ns ns.c
./ns
```

这样我们就进入了这个子进程的shell中，我们可以运行`ls /tmp`和宿主机进行一下对比：

![mount_namespace_show_demo](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/mount_namespace_show_demo.png)

你会发现为啥两边展示的数据会完全一样？因为按照上一部分Cpu Namespace的结论，应该分别看到两个不同的文件目录才对。这是因为`Mount Namespace`修改的，是进程对文件系统“挂载点”的认知，意思也就是只有发生了`挂载`这个操作之后生成的所有目录才会是一个新的系统，而如果不做挂载操作，那就和宿主机的完全一致。那如何解决这个问题呢？其实就是在创建进程时，除了声明一个`Mount Namespace`之外还告诉这个进程需要进行一次挂载操作就可以了，可以简单修改一下新进程的代码，然后运行查看：

``` C
int container_main(void* arg)
{  
    printf("Container - inside the container!\n");
    mount("none", "/tmp", "tmpfs", 0, "");
    execv(container_args[0], container_args);
    printf("Something's wrong!\n");
    return 1;
}
```

![mount_namespace_show_demo_new](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/mount_namespace_show_demo_new.png)

现在就会发现，对文件的隔离已经生效了，并且在子进程内部新创建的目录宿主机中一样看不到了，因为子进程的`/tmp`已经被挂载进了`tmpfs（一个内存盘）`中了，这就相当于创建了完全一个新的`tmp`环境。

为什么要花这么大的篇幅来说这个呢，是因为它就`是Docker镜像的主要技术之一`。上面的栗子，我只是把`tmp`这个目录进行了挂载，同样的，在Cloud Foundry中挂载隔离的文件也都是应用所需要的一些目录，而Docker镜像做的，是把`整个根目录`全部挂载进了这个新的进程中。

> 这里要稍微介绍一下Linux系统地文件结构的知识：和我们平时使用的Windows操作系统不同，Linux操作系统的系统**内核**与**操作系统所包含的文件、配置和目录**是`分开存放的`，系统只有在开机启动时才会加载指定版本的内核镜像。Linux所使用到的文件配置目录被称为`根文件系统（rootfs）`，也就是我们日常操作Linux看到的`/`目录。

Docker镜像的本质，其实也就是对rootfs的一次封装，Docker将一个应用所需操作系统的rootfs，通过`Mount Namespace`进行了封装，这样带来的直接结果就是：**改变了应用程序和操作系统的依赖关系**（这个很类似于强哥上一次讲到的控制反转的理念）即原本应用程序是在操作系统内运行的，而Docker把“操作系统”封装变成了应用程序的依赖库，这样就解决了应用程序运行环境`一致性`的问题，因为不管你是在自己主机上，还是服务器上，又或者是其他什么机器上，因为应用所运行的系统已经成了一个“依赖库”了，所以每个地方运行，这个一致性是完全可以保证的。

但是目前只解决了一致性，复用性的问题还没有解决，即我不可能每制作一个镜像就挂载一个新的rootfs吧，这样先不说复用性了，光硬盘都需要占用很大的空间来实现这些挂载内容。因此，Docker还为镜像使用了`另一个主要技术：UnionFS`以及一个`全新的概念：层（layer）`。

先说一下`UnionFS`是干什么的，UnionFS是一个联合挂载的功能，它可以将多个路径下的文件联合挂载到同一个目录下，举个栗子，现在有一个如下的目录结构：

``` shell
$ tree
.
├── A
│   ├── a
│   └── x
│ 
└── B
    ├── b
    └── x
```

A目录下有a和x两个文件，B目录下有b和x两个文件，通过UnionFS的功能，我们可以将这两个目录挂载到C目录下：

``` shell
$ tree ./C
C
├── a
├── b
└── x
```

最终C目录下的x只有一份，并且如果你对C目录下的a、b、x修改，之前目录A和B中的文件同样会被修改。而Docker正是用了这个技术，对其镜像进行了联合挂载，比如可以分别把`/sys`，`/etc`,`/tmp`目录一起挂载到rootfs中形成一个在子进程看起来就是一个完整的rootfs。

此外，Docker还自己创新了一个层的概念，它将系统内核所需要的rootfs内的文件挂载到了一个`只读层`中，将用户的应用程序、系统的配置文件等之类可以修改的文件挂载到了`可读写层`中，并且在容器启动时可能会用到的初始化参数挂载到了`init层`中。之后，将这三层再次联合挂载，最终形成了容器中的rootfs。这时你应该明白了，这个`只读层`是几乎同样版本的代码几乎不会改变的，比如我们的.net core项目用到的基础环境等等都会设计在了只读层，每次获取最新镜像时，因为每一份只读层都是完全一样的，所以完全不用下载，这也解释了为什么Docker镜像只在第一次下载时那么慢，而之后的镜像都很快，并且明明每份镜像看起来都几百兆，但是最终机器上的硬盘缺没有占用那么多的原因。也是这个技术，解决了镜像复用性的问题。我们可以看一下Docker镜像分层的图：

![docker_layer](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/docker_layer.png)

这里有个问题不知道你有没有考虑过，如果我们的程序做的是删除只读层中的文件，那会发生什么事呢？其实解决方案是：Docker在可读写层加入了一个whiteout的文件夹（BLM政治不正确），所以在只读层做的操作其实就在这个文件夹中加入了一个新的文件，然后在容器中就通过这个文件做了`“逻辑删除”`，只是做了`“遮挡”`的操作，原本的文件还是存在的。

好了，以上就是Docker容器的整个原理了，我们可以总结一下，Docker创建容器的过程其实是：

1. 启用Linux Namespace配置；
2. 设置指定的Cgroups参数；
3. 切换进程的根目录

对于制作一个Docker镜像，Docker使用了联合挂载的功能，和创建了层的概念，解决了镜像的一致性和复用性。

### 对比与思考

其实Docker还做了很多功能，比如权限配置，DeviceMapper等等，这里说的仅仅是一个普及性质的概念性讲解，底层的各种实现还有很复杂的概念。并且或多或少地，你肯定也会有一个想法，这个容器和传统的虚拟机有啥区别？

其实容器技术和虚拟机是实现虚拟化技术的两种手段，只不过虚拟机是通过Hypervisor控制硬件，模拟出一个GustOS来做虚拟化的，其内部是一个几乎真实的虚拟操作系统，内部外部是完全隔离的。而容器技术是通过Linux操作系统的手段，通过类似于Docker Engine这样的软件对系统资源进行的一次隔离和分配，它们之间的对比关系大概如下：

![compare_with_v_machine](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/compare_with_v_machine.jpg)

虚拟机相较于Docker容器来说更加安全，因为其是物理隔离的，但是这带来一个后果：一个运行着 CentOS 的 KVM 虚拟机启动后，在不做优化的情况下，虚拟机自己就需要占用 100~200 MB 内存。此外，用户应用运行在虚拟机里面，它对宿主机操作系统的调用就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘 I/O 的损耗非常大。相反：容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的；而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。

因此，更加符合现在“敏捷”和“高性能"的容器就成为了PaaS这种更需要细粒度资源管理的平台佼佼者。

但是容器的`弊端`也特别明显，因为容器是模拟出来的隔离性，所以对Namespace模拟不出来的资源：比如操作系统内核就完全无法隔离，`容器内部的程序和宿主机是共享操作系统内核的，也就是说，一个低版本的Linux宿主机很可能是无法运行高版本容器的`。还有一个典型的栗子就是`时间`，`如果容器中通过某种手段修改了系统时间，那么宿主机的时间一样会改变`。

`还有一个弊端就是安全性`，一般的企业，是不会直接把容器暴露给外部用户直接使用的，因为容器内可以直接操作内核代码，如果黑客可以通过某种手段修改内核程序，那就可以黑掉整个宿主机，这也是为什么我们组从刚开始自己写Docker到最后弃用的直接原因。现在一般解决安全性的方法有两个：一个是限制Docker内进程的运行权限，控制它值能操作我们想让它操作的系统设备，但是这需要大量的定制化代码，因为你可能并不知道它需要操作什么；另一个方式是在容器外部加一层虚拟机实现的沙箱，这也是阿里云以及美团的主要实现方式，想要具体了解，可以看这篇文档[云原生之容器安全实践](https://tech.meituan.com/2020/03/12/cloud-native-security.html)。

## 大航海时代

### 天下归一

让我们来继续讲故事。

上节说到，Docker项目依靠自己创新的Docker Image瞬间爆红。与此同时，很多厂商也从中发现了商机，纷纷推出自己的容器产品抢占市场，比如CoreOS推出了Rocket（rkt）容器，Google也开源了自己的容器项目lmctfy（Let Me Container That For You）等，但是面对Docker项目的强势，就算是Google也毫无招架之力，因此Google打算和Docker公司开展合作，关停自己的容器项目，并且和Docker公司一同维护开源的容器运行时，但是Docker公司很强势的拒绝了这个明显会削弱自己地位的合作。并且Docker公司此时也意识到了，自己仅仅是云计算技术栈中的幕后英雄，只能当做平台最终部署应用的载体，要想成为这个领域的核心，就应该有自己的生态。因此，在爆火并且有了充足的资金之后，Docker公司开始了疯狂的买买买来扩充自己的实力，其中最出名的就署名`Fig`，它是出名的`Docker Compose`项目的前身。通过这些并购与自身研发迭代，Docker公司最终推出了以自己为核心的PaaS三件套：`Docker Compose`、`Docker Swarm`以及`Docker Machine`，并且通过三件套迅速成为了当时行业内的霸主。此外，Docker公司还做出了一个更加惊人的动作：将公司名由dotCloud改名为Docker，并且将`Docker`注册成了自己的商标，开展自己的商业化路径。但是这样做同时也意味着，其他公司使用Docker就要给Docker公司支付大额的授权费用，这为Docker以后的没落埋下了伏笔。

这里稍微说一下Docker三件套：

1. Docker Compose：Compose的前身是一个仅有两个人维护的Docker容器编排的开源项目Fig，其功能是：假如现在用户需要部署一个应用A和一个数据库B以及负载均衡C，那么Fig就允许用户把ABC三个容器定义在一个配置文件中，并且指定它们的关联关系。最后只需要一条命令`fig up`即可直接使用。在Docker大火的时候，Fig项目在Github上的热度堪比Docker，因此在2015年Docker公司将其收购，并且改名为Docker Compose作为Docker容器的编排工具，并且使用至今
2. Docker Swarm：Swarm是Docker的集群管理项目，其主要的逻辑就和上一节讲过的Cloud Foundry类似，可以以类似于`docker run 我的镜像`的命令行方式，以`docker run -H 我的Swarm集群API地址 我的镜像`这样将Docker运行成一个集群并且进行管理，Swarm的出现将Docker项目从一个普通的容器升级成为PaaS平台
3. Docker Machine：Machine应该是Docker三件套里最不出名的了，我对此也没有过多了解，只知道它的功能是将虚拟机运行成容器，并且可以以管理Docker容器的方式管理虚拟机

然而人红是非多，更何况你还抢了别人的蛋糕，由于Docker公司在Docker开源项目的发展上始终保持着绝对的权威和发言权，并且在多个场合用实际行动挑战到了其他玩家，并且，Docker公司的商业化路径严重侵害了曾今的合作公司RedHat的切身利益，再加之，Docker在2015年间的高速迭代表中现出了各种不稳定的breaking change开始让社区叫苦不迭。于是，容器领域的其他几位玩家开始商讨“切割”Docker项目的话语权。最终，Docker公司迫于压力，于2015年6月22日，牵头了CoreOS、Google、RedHat等公司，共同成立了一个中立的基金会，并将自己的容器运行时LibContainer捐出，改名为RunC，基金会依据RunC共同制定了一套容器和镜像的标准和规范——OCI（Open Container Initiative），OCI的成立，意味着容器运行时和镜像的实现与Docker项目完全剥离，让其他玩家不依赖Docker实现自己的Docker运行时成为可能。

但是，由于成立基金会只是Docker公司对这些大公司的妥协，并且其当时确实维护着数量足够庞大的社区，因此Docker公司对基金会的成立有恃无恐，并且对标准的制定并不关心。因为在当时的大环境下，Docker自己的实现，已经就是业界统一的标准了。

由于OCI基金会发展十分缓慢，因此Google、RedHat等基础设施领域的头部玩家打出了手中的王牌，共同牵头成了一个名叫`CNCF（Cloud Native Computing Foundation）的基金会`（这也是云原生这个概念第一次出现在大众视野内），并且基金会成立的思想基础是，以`Kubernetes项目`为基础，建立一个由开源基础设施领域厂商主导的按照独立基金会方式运营的平台级社区，来对抗以Docker为核心的容器商业生态。也正是这个基金会的成立，我们第二节的主人公`Kubernetes`也开始崭露头角了。

`Kubernetes`是Google公司早在2014年就发布开源的一个`容器基础设施编排`框架，和其他拍脑袋想出来的技术不同，`Kubernetes`的技术是有理论依据的，即：`Borg`。[Borg](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43438.pdf)是Google公司整个基础设施体系中的一部分，Borg是Google公司整个基础设施体系中的一部分，Google也发布了多篇关于Borg的论文作为其理论支持。其上承载了比如MapReduce、BigTable等诸多业界的头部技术。因此Borg系统一直以来都被誉为Google公司内部最强大的“秘密武器”，也是Google公司最不可能开源的项目，而`Kubernetes`就是在这样的理论基础上开发的。下图是Google Omega论文所描述的Google已公开的基础设施栈。

![borg_infr](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/borg_infr.png)

但是，也正由于开发灵感和设计思想来源于Borg，`Kubernetes`项目在刚发布时就被称为`曲高和寡`，太过超前的思想让开发者无法理解，并且也一直由Google的工程师自行维护，所以在发布之初并没有获得太多的关注和成长。

然而，CNCF的成立改变了这一切，RedHat的长处就是有着成熟的社区管理体系，并且也有足够多的工程能力，这使得`Kubernetes`项目在交由社区维护开始迅速发展，并且逐渐开始和Docker分庭抗礼。并且和Docker的封闭商业模式不同，Kubernetes反其道而行之主打开源和民主化，每一个重要功能都给用户提供了可定制化的接口，并且普通用户也可以无权限拉取修改Kubernetes的代码，社区有专门的`reviewer`以及`approver`，只要你写的代码通过PR通过了代码审核和批准，就会成为Kubernetes的主干代码，这也大大的增加了Kubernetes的活力。并且，依托于开放性接口，基于Kubernetes的开源项目和插件比比皆是，并且依托于优秀的架构设计，微服务等新兴的技术理念也迅速落地，最终形成了一个百花齐放的稳定庞大的生态。

面对Kubernetes社区的崛起和壮大，Docker公司不得不承认自己的豪赌以失败告终，2017年开始，Docker将Docker项目的容器运行时部分Containerd捐赠给了CNCF社区，并且在10月宣布将在自己的Docker企业版中内置Kubernetes项目，这也标志着持续了近两年的容器编排之战落下帷幕。2018年1月，RedHat公司宣布斥资2.5亿美元收购CoreOS，2018年3月，这一切纷争的始作俑者Docker公司的CTO Solomon Hykes宣布辞职，至此，纷扰的容器技术圈尘埃落定，天下归一。

### Kubernetes的创新

上文虽然提到了CNCF社区的活力对比Docker生态的封闭获得了大量的成功，但光社区活力并不足以让Docker公司这么迅速的败下阵来，Kubernetes击败Docker的主要原因还是对`容器编排`这项技术的超前理解对Docker公司来说基本就是**降维打击**，毫无还手之力。

从上文我们知道，Docker所构建的，是以Docker容器为最核心的PaaS生态，包括以`Docker Compose`为主的简单容器关系编排，以及以`Docker Swarm`为主的线上运维平台，用户可以通过Docker Compose处理自己集群中容器之间的关系，并且通过Docker Swarm管理运维自己的集群，可以看到这一切其实就是当初Cloud Foundry的PaaS功能，所主打的就是和Docker容器的无缝集成。因此，Kubernetes如果要和Docker对抗，肯定不能再做Docker容器管理这种Docker本身就已经支持的功能了，这样的话别说分庭抗礼，可能连Docker的基本用户都吸引不到。因此在设计之初，就确定了**不以Docker为核心依赖**的设计理念，所以在Kubernetes中，Docker仅是容器运行时实现的一个可选项，用户可以依据自己的喜好任意调换自己想用并且支持了Kubernetes提供接口的任意容器。此外，Kubernetes准确的抓住了Docker容器的一个致命性的弱点进行了自己的创新。

这个弱点是什么我们先不谈，让我们先来看看什么是`容器编排`。所谓容器编排，其实就是处理容器和容器之间的关系，因为一个分布式的大型系统里，不可能是以多个单一个体存在的，它们可能是一个与多个，一群与一群这样相互交织着。Docker Compose做到的是为多个有交互关系建立一种“连接”，把它们全部编写在一个`docker-compose.yaml`文件中，然后统一发布，我后面说到的组里的ELK功能就是这样做的，这样做也有优点，就是对于简单的几个容器之间的交互来说非常便利。但是对于大型的集群来说真的有点杯水车薪了，并且这种每出现一种新需求就单独做一项新功能的开发模式，在后期代码会十分难以维护。

而与Docker这种站在容器视角上只能处理容器之间的关系所不同，Kubernetes所做的是以软件工程的设计理念将关系进行了不同之类的划分，定义了`紧密关系（Pod之间）`和`交互关系（Service之间）`的概念，然后再对不同的关系进行特定的编排实现。乍一听可能是一头雾水，我举一个不太实际但是一看就懂的栗子：如果把容器之间的关系比作人之间的关系，Docker能处理的是`站在单一个体的角度上`处理人与人之间的人际关系，而Kubernetes就是上帝，`站在上帝视角`不仅能处理人与人之间的人际关系，还能处理狗与狗之间的狗际关系，最主要的是能处理人与狗之间的交往关系。

而实现上述`紧密关系`的原理，就是Kubernetes的创新**Pod**了。`Pod是Kubernetes所创新的一个概念，其原型是Borg中的Alloc，是Kubernetes运行应用的最小执行单元，由一个或者多个紧密协作的容器组合而成`，其出现的原因是针对容器的一个`致命性弱点——单一进程`这问题的扩展，让容器有了进程组的概念。通过第一节，我们知道了容器的本质是一个进程，其本身就是超级进程，其他进程都必须是它的子进程，因此在容器中，没有进程组的概念，而在日常的程序运行中，进程组是常常配合使用的，如：

> Linux中有一个负责操作系统日志处理的程序rsyslogd是由三个模块：imklog模块、muxsock模块以及rsyslogd自己的main函数主进程组成。这三个进程组一定要运行在同一台机器上，否则它们之间的基于Socket的通信和文件的交换都会出现问题。

而上述的这个问题，如果出现在Docker中，就不得不使用三个不同的容器分别描述了，并且用户还得自己模拟处理它们三个之间的通信关系，这种复杂度可能比使用容器运维都高的多。并且对于这个问题的运维，Docker Swarm也有自己本身的问题。以上述的栗子为基础，如果三个模块分别都需要1GB的内存运行，如果Docker Swarm运行的集群中有两个node，node-1剩余2.5GB，node-2剩余3GB。这种情况下分别使用`docker run 模块`运行上述三个容器，基于Swarm的`affinity=main约束`，他们三个都必须要调度到同一台机器上，但是Swarm却很有可能先分配两个去node-1，然后剩余的一个由于还剩0.5GB不满足调度而使这次调度失败。这种典型的成组调度（gang scheduling）没有被妥善处理的栗子在Docker Swarm中经常存在。

基于上述的需求，Kubernetes有了`Pod`这个概念来处理这种紧密关系。在一个Pod中的容器共享相同的Cgroups和Namespace，因此它们之间并不存在边界和隔离环境，它们可以共享同一个网络IP，使用相同的Volume处理数据等等，其原理其实很简单，就是在多个容器之间创建其共享资源的链接。但是为了解决到底是A共享B，还是B共享A，以及A和B谁先启动这种拓扑性的问题，一个Pod其实是由一个Infra容器联合AB两个容器共同组成的，其中Infra容器是第一个启动的：

![pod](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/pod.png)

Infra容器是一个用汇编语言编写的、主进程是一个永远处于“暂停”状态的容器，其仅占用极少的资源，解压之后也仅有100KB左右。

现在，让我们看看在Kubernetes中，这样的Pod长什么样吧。我们在任意一个装有Kubernetes的集群中通过以下的yaml文件和shell命令运行处一个Pod，这个yaml文件具体是什么意思暂时不用理会，之后我会对这个yaml做一说明，我们目前只需要明白：`Kubernetes中的所有资源都可以通过以下这种yaml文件或者json文件描述的`，现在我们只需要知道这是一个运行着busybox和nginx的Pod即可：

``` yaml
apiVersion: v1
kind: Pod
metadata:
  name: hello-pod
spec:
  shareProcessNamespace: true # 加入了此参数，容器内才会共享ProcessNamespace
  containers:
  - name: hello-busybox
    image: busybox
    stdin: true
    tty: true
  - name: hello-nginx
    image: nginx
```

创建这个hello-pod.yaml文件之后运行以下命令：

``` shell
# 创建Pod
kubectl apply -f hello-pod.yaml

# 成功后进入执行此命令进入此Pod的busybox shell
kubectl attach -it hello-pod -c hello-busybox

# 在busybox shell中执行
ps ax
```

通过上述命令，我们就成功创建了一个pod，我们可以从执行结果看到infra容器的主进程成为了此Pod的PID==1的超级进程，说明了Pod是组合而成的：

![infra_ps](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/infra_ps.png)

至此，我们应该要理解Pod是Kubernetes的最小调度单位这个概念了，并且也应该把Pod作为一个整体而不是多个容器的集合来看待。

现在，我们回过头来看看描述了这个Pod的文件类型YAML吧。

首先来看看YAML的语法定义：

> YAML是一种专门编写配置文件的语言，其简洁且强大，在描述配置文件方面远胜于JSON，因此在很多新兴的项目比如Kubernetes和Docker Compose等都通过YAML来作为配置文件的描述语言。与HTML相同，YAML也是一个英文的缩写：YAML Ain't Markup Language，聪明的同学已经看出来了，这是一个递归写法，突出了满满的程序员气息。其语法有如下特征：

* 大小写敏感
* 使用缩进表示层级关系，类似Python
* 缩进不允许使用Tab，只允许使用空格
* 缩进的空格数目不重要，只要相同层级的元素左侧对其即可
* 数组用短横线`-`表示
* NULL用波浪线`~`表示

有了以上概念，我们就可以把上述的YAML改写成一个JSON，来看看它们的区别：

``` json
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "name": "hello-pod"
    },
    "spec": {
        "shareProcessNamespace": true,
        "containers": [
            {
                "name": "hello-busybox",
                "image": "busybox",
                "stdin": true,
                "tty": true
            },
            {
                "name": "hello-nginx",
                "image": "nginx"
            }
        ]
    }
}
```

这两种写法在Kubernetes中是等效的，上述的JSON也可以正常运行，但是Kubernetes还是更推荐使用YAML，因此从上面的对比中我们也能发现，之前一直感觉很香的JSON现在看起来也不香了，因此需要些大量的字符串标志。其上只列出了YAML日常开发中用到的常用的功能，具体如果还想继续了解，可以看我最后参考文档中的YAML教程。

看完语法，我再来说说上述YAML中的各个节点在Kubernetes所表示的意思。Kubernetes中的有一种类似于Java语法万物皆对象的概念，所有内部的资源，包括服务器node、服务service以及运行组Pod在kubernetes中皆是以对象的形式存储的，其所有对象都由一下固定的部分组成：

``` yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample
  # ...
spec:
# ...
```

* apiVersion：在官方文档中并没有给出相应的解释，但是从名字可以看出这是一个`规定API版本`的字段，但是此字段不能自定义，必须符合Kubernetes的官方约束，目前我们用到的基本都是v1，表示稳定版
* kind：指明当前的配置是什么类型，比如Pod、Service、Ingress、Node等，注意这个首字母是大写的
* metadata：用于描述当前配置的meta信息，比如name，label等
* spec：是指明当前配置的具体实现

所有的Kubernetes对象基本都满足以上的格式，因此最开始Pod的yaml文件的意思是“使用v1稳定版本的API信息，类型是Pod，名称是hello-pod，具体实现是开启ProcessNamespace，有两个容器，信息分别为…”。

知道了YAML的概念，让我们在回归主题。为了解决容器单一进程只是创建Pod的原因之一，还有一个比较重要的原因是Google通过Pod实现了自己的`容器设计模式`。设计模式大家肯定都知道，一个简单地单例模式连大一的学生都会写，而Google则为Kubernetes编写了适合其开发的容器设计模式，其[论文](https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_burns.pdf)感兴趣的同学也可以自己去学习。

举个最常用的栗子：

> 我们知道Java项目并不能像.Net Core项目那样编译完成后直接自宿主运行，必须要把编译生成的war包拷贝到服务宿主程序比如Tomcat的运行目录下才可以正常使用。但是越大的公司分工越明确，很有可能Java项目和服务宿主程序不是同一个人甚至同一个部门开发的，因此为了让他们各自独立开发并且还可以紧密合作，我们可以把它们同时写在一个Pod中。

下面这个yaml文件就定义了一个满足上述需求的Pod：

``` yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-java
spec:
  initContainers:
  - image: sample-java:latest
    name: war
    command: ["cp", "/sample.war", "/app"]
    volumeMounts:
    - mountPath: /app
      name: sample-volume
  containers:
  - image: sample-tomcat:latest
    name: tomcat
    command: ["sh", "-c", "/root/apache-tomcat/bin/start.sh"]
    volumeMounts:
    - mountPath: /root/apache-tomcat/webapps
      name: sample-volume
    # ports: 
  # ... ...
  volumes:
  - name: sample-volume
    emptyDir: {}
```

在这个yaml文件中，我定义了一个java程序和tomcat程序的容器，并且对这两个容器之间的容器进行了一次挂载操作：将java程序的/app路径以及tomcat程序的/root/apache-tomcat/webapps同时挂载到了sample-volume这个挂在卷上，并且最后定了这个挂载卷就是一个内存数据卷。并且定义了java程序所在的容器是一个initContainer，说明此容器是在tomcat容器之前启动的，并且启动之后执行了一个cp的命令。

> 综上，我们可以看到上树的Pod描述了这样一个场景：程序运行开始运行时，Java容器启动，把自己的war包sample.war拷贝到了`自己的/app目录`下；之后tomcat容器启动，执行启动脚本，执行的war包从`自己的/root/apache-tomcat/webapps`路径下获得。

可以看到通过上述的配置描述，我们既没有改动Java程序，也没有改动tomcat程序，却让它们完美的配合工作了，完成了解耦操作。这个栗子就是容器设计模式中的`Sidecar模式`，还有很多设计模式，感兴趣的同学可以去[《Kubernetes与云原生应用》之容器设计模式](http://www.dockerinfo.net/2067.html)学习使用。

以上就是Kubernetes为了解决`紧密关系`而抽象出来的概念Pod的基础内容了，需要注意的是，Pod提供的只是一种编排的思想，而不是具体的技术方案，在我们使用的Kubernetes框架中，Pod只不过是以Docker作为载体实现了而已，如果你使用的底层容器是虚拟机比如[virtlet](https://github.com/Mirantis/virtlet)，那这个Pod创建时就根本不需要那个Infra Container，因为虚拟机天生就支持多进程协同。

### 容器的编排大戏

然而，光有了Pod是不能够解决用户部署应用的需求的，大部分用户用户所关心的是如何调度管理自己的应用，这部分功能才是Kubernetes最强大的杀器。比如现在用户的需求是：

> 以3机负载均衡的形式部署一个私有云客户的Forguncy应用，这应该如何实现呢？

这项工作如果在我们之前自己开发的版本中实现起来大概是这样的：需要三个不同的物理机，在这三个物理机上分别安装把用户的应用run成容器，然后在三台服务器之外再买一个负载均衡服务，然后通过域名解析配置，将流量分别导向三个不同的服务机从而达到上述的需求。用一句话解释看起来好像我们自己做也挺简单的，但是，如果我们只有两台服务器呢？如果有一台中的container挂了呢？如果两台服务器CPU跑满了呢？这种调度方面的东西看起来很简单，但是实现起来却需要长时间的编码和调试，并且最终做出来可能也就是个Docker Swarm而已。

我们把这个需求看做是我们最终的目标，先让我们来一步一步看看kubernetes中关于编排方面的处理，相信大家看完这部分内容之后，就能很容易完成上述的需求了。

容器编排在kubernetes中其实就演变成了Pod编排，怎么让这些Pod配合起来协同工作就是编排的核心了。上一节我们知道了，kubernetes将各种关系进行了抽象化，这些关系其实可以看做是Pod之间的关系。kubernetes将Pod的关系抽象成了一下几种，并且为这些关系定义了相对的控制器进行编排管理：

* 无状态Pod副本之间的协同关系——Deployment
* 有状态Pod副本之间的拓扑关系——StatefulSet
* 容器化守护进程——DaemonSet
* 离线业务——Job和CronJob

这些概念看起来云里雾里不知所云，什么有状态无状态的，其实就是通过上述的控制器对Pod的不同管理方式而已。这里限于篇幅和个人水平，我只讲一下我们云组使用到的也是kubernetes最常用的一种控制器Deployment。

Deployment控制器的功能是：维护多个相同的无状态Pod副本`以规定的数量运行`，并且支持`水平扩展`以及`滚动更新`。所以，我们最终需求中的负载均衡中的Forguncy服务的Pod，就可以通过Deployment管理，我们可以通过Deployment让我们的Pod在kubernetes集群中始终以3个副本的形式存在。

这里引出了一个概念：`控制器`，控制器是kubernetes中管理待编排对象的程序，其把这种`一个对象管理另一个对象的模式称为控制器模式`，kubernetes中的所有待编排对象都是通过控制器模式管理的。其核心就是一个死循环，在循环中不停地判断当前编排对象的状态，如果不满足预期状态就更新它，如下的伪代码就是描述一个控制器的工作原理：

``` go
for {
  实际状态 := 获取集群中对象X的实际状态（Actual State）
  期望状态 := 获取集群中对象X的期望状态（Desired State）
  if 实际状态 == 期望状态 {
    什么都不做
  } else {
    执行编排动作，将实际状态调整为期望状态
  }
}
```

因此我们知道了，如果定义我们的Pod使用Deployment来编排，并且要求副本数量是3，那Deployment控制循环中就会不停地判断我们的Pod的副本数量是否是3，如果不是，就会触发水平扩展功能进行调整，最终达到满足期望状态（副本数==3）。

说了这么多，我们可以通过一个栗子看看Deployment是如何工作的。由于Forguncy的镜像配置过于复杂，因此这里我们通过一个Nginx的多副本配置来感受一下Deployment控制器的控制结果，我们可以通过以下yaml定义一个维护了3个nginx副本的deployment：

``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-deployment-nginx
spec:
  selector:
    matchLabels:
      app: sample-deployment-nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: sample-deployment-nginx
    spec:
      containers:
      - name: sample-nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
```

这里我们看到了三个特殊字段：

1. selector：是一个选择器，就可以理解为js中的选择器，其功能就是选择指定的pod运行，这个栗子中我们指定所有app==sample-deployment-nginx的pod才会被这个Deployment所部署
2. replicas：指明这个Deployment维护的副本个数
3. template：控制器中提供了template这个语法，可以让我们直接在控制器的yaml中直接编写所需要编排的Pod信息

编写完这个sample-deployment-nginx.yaml后，执行一下：

``` shell
kubectl apply -f sample-deployment-nginx.yaml
```

这个三副本的控制器就被成功运行了，我们可以通过这个命令进行查看：

``` shell
kubectl get pods -l app=sample-deployment-nginx
```

![deployment_pods_running](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/deployment_pods_running.png)

可以看到我们的3副本Pod已经成功在kubernetes中运行了，此时，如果你执行以下命令删除podname==sample-deployment-nginx-54545f95cd-wtllm的副本：

``` shell
kubectl delete pod sample-deployment-nginx-54545f95cd-wtllm
```

它还是会自动生成一个新的pod来维持这个replicas==3：

![deployment_pods_recovering](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/deployment_pods_recovering.png)

通过上述例子，我们可以看到Deployment控制器对副本数量的控制结果，其实，对副本数的控制是ReplicaSet控制器的结果，Deployment是ReplicaSet控制器的控制器，这种多层之间相互控制的模式在kubernetes也十分常见，其之间的关系如下图所示：

![deployment_replicaSet_pods](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/deployment_replicaSet_pods.jpg)

此时，我们再来看水平扩展/收缩，我们可以执行以下命令，分别将这个deployment对象的信息修改成5/0，并且最终改为3：

``` shell
kubectl edit deployment/sample-deployment-nginx
```

我们可以看到如下结果，deployment将我们收缩的成0的5个pod副本都删除并且重新创建了3个新的副本：

![deployment_editing](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/deployment_editing.png)

至此，水平扩展和收缩的功能就已经成功了，最后我们来看看滚动更新，我们继续通过edit命令将nginx的版本从1.9.1改为latest，我们可以看到一个滚动更新的过程：

![deployment_updating](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/deployment_updating.png)

也正是这样逐步更新删除的动作，被称为滚动更新，其更新过程中，deployment和replicaSet的关系如下图：

![deployment_replicaSet_pods_updated](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/deployment_replicaSet_pods_updated.jpg)

至此，一个deployment管理pod的所有功能都已经展示完成了，可以看到kubernetes对这种控制器管理之间精妙的设计，多个控制器协同工作，这样既能保证精准也能拆分进程阻塞提升性能。当你理解了deployment控制器，对于其他的控制器来说也就很容易理解了，在此我来简单说明一下其他控制器的控制逻辑：

1. StatefulSet：控制满足有拓扑状态或者持久化存储的Pod，拓扑状态的意思就是Pod之间存在明确的先后生成关系，持久化存储就是当副本被删除或者修改了，其内部保存的数据还会存在
2. DaemonSet：守护进程控制器，是一个Node（服务器节点）仅能存在一个的Pod，比如系统的日志采集器等就应该用这种方式调度
3. Job与CronJob：Job就是任务调度，一个Pod在调度完成后就结束了不会再有新的任务产生，Job用于维护一个任务Pod运行中的各种状态正常，异常状态重启等。对应的CronJob就是定时任务，使用过Quartz的肯定不陌生

综上，kubernetes中就是通过上述的各种控制器维护所有Pod的编排工作的，并且其还提供了完善的API可以让用户自行定义满足自己需求的各种Pod编排控制器。但是对于deployment我只是简单的展示了一些常用的功能点，其内部还有滚动更新的最大资源、金丝雀发布和灰度发布等各种功能需要细致的学习。

### 我的服务在哪？

上面我说了，需要完成我们最终发布forguncy应用的需求，编排只是其功能的一部分，而另一部分就是服务的发现机制了。目前，我们仅仅是把我们的服务发布在了kubernetes集群中，但是要想内部交互，外部调用完成`人与狗的交往关系`还缺少服务发现这个功能。服务发现这个功能做过微服务的同学肯定了解过，spring cloud项目中的Eureka框架就是完成这个功能的，其主要工作就是注册内部的服务，以供其他集群的服务可以调用访问这个服务。

kubernetes中野存在这样的功能（而且很有可能是kubernetes现有才激发了各种微服务框架产生服务发现机制），对应的模块是Service与Ingress，我们来分别说这两个功能。

Service类似于服务的注册功能，其逻辑很简单，在kubernetes声明一个服务，从而生成一个VIP（虚拟网络），所有Kubernetes集群中的其他组件，都可以通过这个VIP来访问这个服务，并且这个服务是不会随Service的改变而改变的，只要创建就是终生存在。而服务的内容是什么呢？其实和上述的Deployment一样，是通过selector选择器确定的。我们可以通过下述yaml来创建一个服务：

``` yaml
apiVersion: v1
kind: Service
metadata:
  name: hostnames
spec:
  selector:
    app: hostnames
  ports:
  - name: default
    protocol: TCP
    port: 80
    targetPort: 9376
```

通过上一节的讲解，我们能知道这个服务所需要的代理的内容是app==hostnames的Pod，并且这里也有一个新的字段`ports`，这个字段是说明该代理的服务的请求方式（protocol）、对外暴露的端口（port）、内部的端口（targetPort）分别是什么。

我们可以通过这个sample-service.yaml的文件创建一个Service并且查看一个Service：

``` shell
# 创建
kubectl apply -f sample-service.yaml

# 查看
kubectl get services hostnames
```

![service_created](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/service_created.png)

可以看到这个service存在一个ClusterIP，这个IP就是这个Service生成的VIP，在集群内部的其他成员，都可以通过这个VIP来访问这个Service。当然，由于我们现在没有任何的具体服务让这个Service代理，因此现在请求这个IP是不会成功的。

接下来，我们为这个Service创建一个具体实现：以下的sample-deployment.yaml文件是创建一个多副本的Pod，其Pod的功能是返回自己的podname：

``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hostnames
spec:
  selector:
    matchLabels:
      app: hostnames
  replicas: 3
  template:
    metadata:
      labels:
        app: hostnames
    spec:
      containers:
      - name: hostnames
        image: k8s.gcr.io/serve_hostname
        ports:
        - containerPort: 9376
          protocol: TCP
~                        
```

可以看到，我们把容器的9376端口暴露的出来，以为这个Pod是通过这个端口与外部通行的。同样我们执行以下命令创建和查看这个pod副本：

``` shell
# 创建
kubectl apply -f sample-deployment.yaml

# 查看
kubectl get pods -l app=hostnames
```

![service_deployment_created](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/service_deployment_created.png)

可以看到这个pod副本已经创建成功了，此时，根据我上一节所说的控制器模式，Service也有对应的处理Service的控制器，其在内部发现了有满足app==hostnames的服务，即将这个服务和Service进行了绑定，此时，我们就可以通过任意一台集群内的主机来请求刚才那个ClusterIP了：

![service_requesting](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/service_requesting.png)

可以看到，我请求了多次，但是每次返回的结果不同，这是因为Service在其内部通过网络插件（CNI）做了负载均衡处理，所以我们最终需求的负载均衡功能，就可以通过这个Service来实现。

在此要说一个我之前一直的误解：我之前一直以为Service是必须对应Deployment这种Pod的编排控制器对象才能工作的，所以把Service --> Deployment --> Pods这条逻辑关系熟记于心，但是其实是**错误的**。在Kubernetes中，每个功能组件各司其职，他们只会处理自己该做的事，比如这里，Service绑定Pod所依赖的是选择器中的app==hostnames，而这个定义是在Deployment中定义在Pod中的，因此Service和Deployment完全没有关系，它们俩谁也不认识谁，关系可以用下图来描述：

![service_deployment](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/service_deployment.png)

并且，之前还错误的任务负载均衡服务是由Deployment提供的，其实这个功能是Service中的网络插件来处理的，并且用户同样也可以自定义使用的网络查件或者负载均衡算法是什么，Kubernetes给了用户足够大的自由度。

在有了Service之后，我们的服务就可以在集群中随意访问达到服务之间的交流关系了。但是要想让我们的服务让最终的用户访问到，我们还需要最后一个组件Ingress。Ingress是Kubernetes中的反向代理服务，它可以解析配置的域名指向到我们内部的Service中，其定义可以通过下述的yaml来实现：

``` yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: sample-ingress
spec:
  rules:
  - host: hostname.sample.com
    http:
      paths:
      - path: /
        backend:
          serviceName: hostnames
          servicePort: 80
```

可以看到，我们将`hostname.sample.com`这个域名指向了我们刚才定义的`hostnames`这个Service，这样，我们的服务就可以通过定义的域名配置供外部的服务进行访问了。而Ingress的创建命令，也和前面所说的一样：

``` shell
kubectl apply -f sample-ingress.yaml
```

有了这部分配置，我们的功能**原则上**就能够让外部访问了，这里加了原则上，是因为我本地没有可供测试的环境，本地的Kubernetes环境是通过kindD生成的，其核心不是多台机器而是多个Docker Container，其在Container内部运行Docker模拟了Kubernetes的功能，因此这也是我全文中**唯一没有验证成功的一个功能模块**。

至此，整个Kubernetes的基础使用的流程就已经介绍完毕了。我们可以看到一个服务在Kubernetes变成Pod，通过Deployment部署，通过Service服务发现，通过Ingress反向代理的全过程，经过这些模块的协力配合之后，我们的forguncy应用终于可以部署在这个Kubernetes集群中了。

### Kubernetes总览

现在，让我们在回过头来看看Kubernetes的内部组件图，我想大家就应该了然于胸了：

![kubernetes_modules](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/kubernetes_modules.png)

还有几个我没有提到的功能模块：

* ConfigMap是用来存储用户配置文件定义的，通过其内部的Volume投射技术实现，其实也是Volume挂载的一种方式，感兴趣的同学可以自己查看
* Secret是用来定义密码类型数据的，同样通过Yaml文件的形式定义在Kubernetes集群中，供需要使用到的Pod通过secretName的方式使用
* PV，PVC是Kubernetes中持久化数据卷的实现方式，它是StatefulSet的核心功能，也是Pod持久化的必要手段，Kubernetes通过PV和PVC拆分，达到了功能点的解耦，这里面也有很多的功能点，感兴趣的同学可以自己在官方文档中学习

其实，Kubernetes集群远比我们目前看到的复杂的多的多，其内部还有大量的功能由于我们不会使用所以我在之前没有介绍到，在这里，我对这些深层次的功能做一些总结，也对深入学习Kubernetes开一个头。

#### Kubernetes组件

首先，我们要知道Kubernetes是有一个或者多个`Master节点`控制多个宿主机实现集群的。其中的宿主机就是我们平时开发中的服务器，在Kubernetes集群中被称为`Node节点`，而整个Kubernetes的核心调度功能基本都在Master节点上。Kubernetes的主要功能通过五个大组件组成：

1. kubelet：安装在Node节点上，用以控制Node节点中的容器完成Kubernetes的调度逻辑
2. ControllerManager：是我上述所讲的控制器模式的核心管理组件，管理了所有Kubernetes集群中的控制器逻辑
3. API Server：服务处理集群中的api请求，我们一直写的kubectl，其实就是发送给API Server的请求，请求会在其内部进行处理和转发
4. Scheduler：负责Kubernetes的服务调度，比如控制器只是控制Pod的编排，最后的调度逻辑是由Scheduler所完成并且发送请求给kubelet执行的
5. Etcd：这是一个分布式的数据库存储项目，由CoreOS开发，最终被RedHat收购成为Kubernetes的一部分，它里面保存了Kubernetes集群中的所有配置信息，比如所有集群对象的name，IP，secret，configMap等所有数据，其依靠自己的一致性算法可以保证在系统中快速稳定的返回各种配置信息，因此这也是Kubernetes和心中的核心组件

#### 定制化功能

其次，Kubernetes给用户提供了极高的自由度，从上述的各种栗子都有不同的展现。其实，Kubernetes给用户提供了三个公开的接口，分别是：

1. CNI：Container Networking Interface，容器网络接口，其定义了Kubernetes集群所有网络的链接方式，整个集群的网络都通过这个接口实现。只要实现了这个接口内所有功能的网络插件，就可以作为Kubernetes集群的网络配置插件，其内部包括宿主机路由表配置、7层网络发现、数据包转发等等都有各式各样的小插件，这些小插件还可以随意配合使用，用户可以按照自己的需求随意定制化这些功能
2. CSI：Container Storage Interface，容器存储接口，定义了集群持久化的一些规范，只要是实现这个接口的存储功能，就可以作为Kubernetes的持久化插件
3. CRI：Container Runtime Interface，容器运行时接口，其实现是Kubernetes的容器运行时，比如默认配置的Docker就是这个集群的容器运行时，用户可以自由选择实现了这个接口的其他任意容器项目，比如之前提到过的containerd和rkt

这里还要重点说一下CRI，Kubernetes的默认容器是Docker，但是由于项目初期的竞争关系，Docker其实并不满足Kubernetes所定义的CRI规范，那怎么办呢？为此，Kubernetes专门为Docker编写了一个叫`DockerShim`的组件，即Docker垫片，用来把CRI请求规范，转换成为Docker操作Linux的OCI规范（对，就是第二部分提到的那个OCI基金会的那个规范）。但是这个功能一直是由Kubernetes项目维护的，只要Docker发布了新的功能Kubernetes就要维护这个DockerShim组件。于是，也就是这个月初的消息，[Kubernetes将在明年的版本v1.20中删除删除DockerShim组件](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/)，这也意味着，从明年的新版本开始，Kubernetes将全面不支持Docker容器的更新了。当然，这对我们普通开发者来说可能并没有什么影响，可能最坏的结果就是我们的镜像需要从Docker换成其他Kubernetes支持的容器镜像。不过根据这几天看到的各个云平台所说的消息，他们都会提供对应的转换措施，比如我们还是提供Docker镜像，平台在发布运维的时候会把这些镜像转换成其他镜像，又或者他们会自行维护一个DockerShim来支持Docker，总之是由解决方案的。但是这个消息同时也告诉了我们，`Docker离灭亡已经不远了`。

#### 架构总览与总结

最后，让我们看看Kubernetes的架构图：

![kubernetes_architecture](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/kubernetes_architecture.png)

通过这一系列的学习，我作为一个普通程序员，不得不赞叹Google这种公司对编码这件事得深厚与极致，框架中因为太多仅因为解耦而产生的组件，并且还提供了这么大的自由度，不得不说是我工作以来学习过的最高端的一个框架。

但是也不得不说明一点，过高的自由度也有负面作用，就是Kubernetes集群部署的复杂度非常高，部署一个满足生产环境需求的Kubernetes框架更是难上加难，网上还有专门卖Kubernetes生产环境集群部署的脚本程序，可见Kubernetes系统的庞大。当然作为普通开发者，我们学习的时候可以使用kinD或者minikube在本地以Docker的形式模拟一个Kubernetes集群，但是离生产环境还差的很远。

## 总结

这篇文档，详细的描述了我们云组上天过程中碰到的几个难啃的神仙。并且我一步一步抽吸剥茧地向大家解释了一个云平台，从最初的虚拟机，到PaaS雏形，到Docker容器化，再到最终Kubernetes的形态的过度和演变。我认为人的记忆是需要依赖于前驱节点的，我不想仅仅通过一篇讲述Kubernetes技术点的文章来一个一个解释Kubernetes中那些难记的名词，而是想让大家一步一步了解整个云生态的演化过程，从而最终理解整个项目。

最后想送给大家一句话：

> 纸上得来终觉浅，绝知此事要躬行

我在第一遍看完这些文档之后觉得自己已经啥都懂了，但是等到自己要写文档的时候真是两眼一抹黑，太多的知识点只停留在听说过不知道是什么的阶段，因此还是建议大家动起手来，一步一步操作一下里面的实例，我相信在自己动手写过一遍之后，你会有不一样的理解~

## 参考文档

* 极客时间：[深入剖析Kubernetes](https://time.geekbang.org/column/intro/100015201)、[Elasticsearch核心技术与实践](https://time.geekbang.org/course/intro/100030501)
* CoolShell：[Linux PID 1 和Systemd](https://coolshell.cn/articles/17998.html)、[DOCKER基础技术：LINUX NAMESPACE（上）](https://coolshell.cn/articles/17010.html)
* Wikipedia：[文件系统层次结构标准](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard)
* [Linux 线程实现机制分析](https://www.ibm.com/developerworks/cn/linux/kernel/l-thread/index.html)
* [YAML 语言教程](http://www.ruanyifeng.com/blog/2016/07/yaml.html)
* [《Kubernetes与云原生应用》之容器设计模式](http://www.dockerinfo.net/2067.html)