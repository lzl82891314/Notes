# 云项目组“上天”过程中发现的那些“神仙”

来云组已经两个月有余了，在我们组上天的过程中，碰到了很多之前开发中完全用不到的技术知识点，也就是我所说的“神仙”，今天我就来给大家盘一盘这些个“神仙”到底都是些什么玩意。

## 继续从PaaS说起

### 容器的兴起

强哥在之前的某次sprint技术分享会中给大家讲过了PaaS和SaaS等概念，那我这次就继续从PaaS开始，先来讲讲他的前世今生吧。

早在2013年前后，服务端的技术是云端计算的天下，当时有大量的云计算厂商涌入市场，比较著名的有`AWS`和`OpenStack`，他们将虚无缥缈的云端计算这种高大上的概念，固化成了实实在在的虚拟机技术，当时的主流用户，也都是将自己的应用上传到云端的虚拟机中，以之前管理物理机的方式管理着可能是一群云端的虚拟服务器。

而就在这个时候，一家主打开源PaaS项目的平台`Cloud Foundry`诞生了。与传统的提供云上虚拟机服务不同，Cloud Foundry的PaaS所提供的服务是`应用托管`的功能，也就是用户只需要通过一条简单的命令比如：`cf push "我的应用"`，就可以把自己的项目部署在可能成千上万台的终端服务器上。然后平台对这些项目提供分发，灾备，监控，重启等等服务（这其实也是我们组想要给用户提供的最终服务），这些托管服务解放了开发者的生产力，让他们不用再关心应用的运维状况，而是专心开发自己的应用。这就是PaaS的概念，平台即服务。

因此，像Cloud Foundry这样的PaaS项目，最核心的组件就是对一套应用的打包和分发机制，Cloud Foundry为每一种主流的语言都定义了一套打包的方式。而cf push命令，就是将应用打好的包上传到云端的存储中，然后Cloud Foundry调度出一个可运行的虚拟机，通知该虚拟机上的Agent下载并且运行这个打包结果。由此可见，Cloud Foundry的虚拟机中会运行多个用户的多个服务，为了解决服务之间相互隔离的边界问题，Cloud Foundry使用了Linux的Namespace技术对运行的应用进行了隔离和分组，使用Cgroup技术对隔离的应用分配资源，而这也正是容器技术的一部分。

由此我们可以知道，容器技术并不是Docker创建的，而且在Docker兴起之前，就已经被其他公司商用了，但是为什么现在一谈起容器，所有人第一时间想到的就是Docker呢？

上面，我提到了Cloud Foundry的部署需要用户对其应用进行打包，然而就是这个打包的功能，成了Cloud Foundry的一个软肋一直被用户诟病：为了上传PaaS，用户就不得不为每一种语言，每一种框架，甚至是每个版本应用维护一个打好的包，这种打包的方法是毫无章法的，还有可能出现本机运行成功，打了个包上传上去之后就无法运行的情况。然而就在这个时候，一个开源项目在社区的热度开始逐渐升温了，这个项目就是第一节的主人公`Docker`。

Docker是一个当时还叫dotCloud的公司开发的容器项目，在开源的短短几个月后就迅速崛起，并一举将Cloud Foundry赶出了局，然而最可笑的是，在Docker刚开源的时候，Cloud Foundry的首席产品经理 James Bayer就在社区做了一次详细的对比，告诉用户Docker和Cloud Foundry一样是一个使用了Namespace和Cgroup技术的沙箱而已，没什么值得关注的。事实上，Docker也确实就和他所说的一样，但是只做了一点小小的创新，而就是这一点小小的创新，对Cloud Foundry造成了毁灭性的降维打击。这个创新就是`Docker镜像`。

Docker镜像几乎完美地解决了Cloud Foundry对于打包方面的软肋。所谓的镜像，其实也是一个压缩包，但是比起Cloud Foundry那种执行文件+启动脚本的打包结果，镜像提供给用户的是一套完整的运行环境，每一个镜像都可以指定操作系统版本，内部可以构建程序执行的文件结构，并且一份镜像可以完全共享多处使用。Docker给用户提供了一套完善的镜像制作流程，用户也就不用关心自己的语言和框架了，只需要定制对应程序所需要的运行的操作系统环境即可（这一点其实也可以简单的看成是对压缩包这种应用做了底层的抽象，比较契合软件设计的原则。也可以看到，软件行业的一点创新，带来的就是降维级别的打击）。总结一下就是：`Docker 镜像完美解决了两个问题：1是本地环境和服务器环境的差异问题；2是一份镜像所有的机器都可以运行的复用问题`。

在这之后，PaaS的市场，已经完全是Docker的天下了。而之后的故事，我会在下一节再继续讲。

### 一个进程的诞生

好了，讲了半天的故事了，也要说一点干货了：容器到底是个什么玩意，能有这么大的市场和这么多服务端程序员对其青睐？先别着急，我先从一个进程说起。

我们都知道，计算机里运行的程序其实都是一个一个的进程，而一个进程其实就是程序执行之后，从磁盘的二进制文件，到内存、寄存器、堆栈指令等等所用到的相关设备状态的一个集合，是`数据和状态综合的动态表现`。而容器技术，其实就是对一个进程的状态和数据进行的一系列`隔离`和`限制`后的结果。因此可以知道，`容器的本质其实就是Linux中的一个特殊进程`。

先来说说隔离，我们可以在任意一个装有Docker程序的Linux中执行下列的命令创建一个简单的镜像：

``` shell
$ docker run -it busybox /bin/sh
```

这条语句的大概意思是：用docker运行一个容器，容器的镜像名称叫`busybox`，并且运行之后需要执行的命令是`/bin/sh`，而`-it`参数表示需要使用标准输入`stdin`和分配一个文本输入输出环境`tty`与外部交互。通过这个命令，我们就可以进入到一个容器内部了，分别在容器中和宿主机中执行`top`命令，可以看到以下结果：

![host_container_comparer](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/host_container_comparer.png)

可以发现，容器中的运行进程只剩下了两个，一个就是主进程PID==1的/bin/sh，另一个就是我们运行的top，而宿主机中的其余的所有进程在容器中都看不到了，这就是`隔离`。本来，每当我们在宿主机上运行一个/bin/sh程序，操作系统都会给它分配一个进程编号，比如PID==100。而现在，我们要通过Docker把这个/bin/sh程序运行在一个容器中，这时候，Docker就会在这个PID==100创建时施加一个“障眼法”，让他永远看不到之前的99个进程，这样运行在容器中的程序就会当自己是PID==1的主进程。

> 这里说明一下，在Linux操作系统中，PID==1的进程被称为超级进程，它是整个进程树的root，负责产生其他所有用户进程。所有的进程都会被挂在这个进程下，如果这个进程退出了，那么所有的进程都被 kill

而这种机制，其实就是对被隔离的程序的进程空间做了手脚，虽然在容器中显示的PID==1，但是在原本的宿主机中，它其实还是那个PID==100的进程。所使用到的技术就是Linux中的`Namespace机制`。而这个机制，其实就是Linux在创建进程时的一个可选参数。在Linux中，创建一个线程的函数是（这里没写错就是线程，Linux中线程是用进程实现的，所以可以用来描述进程）：

``` shell
int pid = clone(main_function, stack_size, SIGCHLD, NULL);
```

如果我们给这个方法添加一个参数比如`CLONE_NEWPID`：

``` shell
int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL);
```

那么这个新的进程就会看到一个全新的进程空间，在这个空间里，它自己的PID就等于1。

`除了刚才说的PID Namespace，Linux操作系统还提供了Mount、UTS、IPC、Network和User的Namespace，用来对各种不同的进程上下文进行隔离操作`。

以上，也就是Linux容器最基本的隔离的实现了。

说完了隔离，再来说说限制。容器的隔离性是通过Linux的`Cgroup`实现的。`Cgroup`的全称是`Linux Control Group`，是Linux操作系统中用来`限制一个进程组使用资源的上限，包括CPU、内存、磁盘、网络带宽等`的功能。在Linux中，Cgroup给用户暴露的API是文件系统，因此用户可以通过修改文件的值来操作Cgroup功能。在Linux系统（Ubuntu）中可以执行以下命令查看CgroupAPI文件：

``` shell
mount -t cgroup
```

![cgroup_file](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cgroup_file.png)

从上图可以看到，系统中存在包括cpu、内存、IO等多个Cgroup配置文件。我们可以以CPU为例来说明以下Cgroup这个功能。对CPU的限制需要引入两个参数`cfs_period`和`cfs_quota`，这个强哥肯定知道当时为了给Docker内的程序限制CPU会经常操作这两个参数，这两个参数是组合使用的，意思是在长度为cfs_period时间内，程序组只能分到总量为cfs_quota的CPU时间。也就是说`cfs_quota / cfs_period == cpu使用上限`。

举个栗子，在`/sys/fs/cgroup/cpu`目录下，执行以下命令创建一个文件夹container：

``` shell
/sys/fs/cgroup/cpu/ > mkdir container
```

可以发现系统会自动为container目录下生成一系列的CPU限制的参数文件，这是Linux系统自动生成的，表示我们成功为CPU创建了一个控制组container：

![cpu_limit_file](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_limit_file.png)

然后我们可以执行一下以下脚本创建一个死循环程序：

``` shell
while : ; do : ; done &
```

会看到返回的`进程为398`，因为死循环所以top可以看到cpu占用率为100%：

![cpu_full](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_full.png)

这时，我们可以看下container目录下的`cpu.cfs_quota_us`和`cpu.cfs_period_us`：

![cpu_usage_file](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_usage_file.png)

`cfs_quota_us`为-1说明并没有限制CPU的运行上限，现在我们可以改一下这个值：

``` shell
echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
```

然后将之前的`进程398`写入这个控制组的`tasks`文件中：

``` shell
echo 398 > /sys/fs/cgroup/cpu/container/tasks
```

这时，我们可以再top一下，发现刚才的死循环的CPU使用率只有20%了：

![cpu_usage_after](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_usage_after.png)

以上，就是通过Cgroup功能对容器做限制的原理了，同理可以用此方法，对一个容器的内存、带宽等做限制，这样，一个简单的容器基本就可以展现在你面前了，Cloud Foundry为自己的PaaS平台为每一个应用所创建的就是这样的一个容器。

> 这里要做一个特别的说明，只有Linux中运行的容器是通过对进程进行限制模拟出来的结果，Windows和Mac下的容器，都是通过Docker Desktop操作虚拟机模拟出来的真实的虚拟容器。

### Docker做了些什么？

从第一节我们知道，Docker通过自己的`Docker 镜像`功能迅速取代了Cloud Foundry，那这个Docker镜像到底是什么呢？先卖个关子，我们先来看看之前说过的隔离功能中的另一个机制`Namespace机制`。

`Mount Namespace`从名字你们也知道这个是和文件挂载相关的，是用来隔离进程的挂载目录的，我们可以通过一个“简单”的栗子来看看它是怎么工作的：

``` C
#define _GNU_SOURCE
#include <sys/mount.h> 
#include <sys/types.h>
#include <sys/wait.h>
#include <stdio.h>
#include <sched.h>
#include <signal.h>
#include <unistd.h>
#define STACK_SIZE (1024 * 1024)
static char container_stack[STACK_SIZE];
char* const container_args[] = {
    "/bin/bash",
    NULL
};

int container_main(void* arg)
{  
    printf("Container - inside the container!\n");
    execv(container_args[0], container_args);
    printf("Something's wrong!\n");
    return 1;
}

int main()
{
    printf("Parent - start a container!\n");
    int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL);
    waitpid(container_pid, NULL, 0);
    printf("Parent - container stopped!\n");
    return 0;
}
```

上面是一个“简单的”的C语言代码，如果刨去现在已经不记得怎么写C语言了之外，这个代码其实真的挺简单的，只有两个逻辑：1是在main函数中创建了一个子进程，并且传递了一个参数`CLONE_NEWNS`，这个参数就是用来实现`Mount Namespace`的；2是在子进程中调用了`/bin/bash`命令运行了一个子进程内部的shell。让我们编译并且执行一下这个程序：

``` shell
gcc -o ns ns.c
./ns
```

这样我们就进入了这个子进程的shell中，我们可以运行`ls /tmp`和宿主机进行一下对比：

![mount_namespace_show_demo](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/mount_namespace_show_demo.png)

你会发现为啥两边展示的数据会完全一样？因为按照上一部分Cpu Namespace的结论，应该分别看到两个不同的文件目录才对。这是因为`Mount Namespace`修改的，是进程对文件系统“挂载点”的认知，意思也就是只有发生了`挂载`这个操作之后生成的所有目录才会是一个新的系统，而如果不做挂载操作，那就和宿主机的完全一致。那如何解决这个问题呢？其实就是在创建进程时，除了声明一个`Mount Namespace`之外还告诉这个进程需要进行一次挂载操作就可以了，可以简单修改一下新进程的代码，然后运行查看：

``` C
int container_main(void* arg)
{  
    printf("Container - inside the container!\n");
    mount("none", "/tmp", "tmpfs", 0, "");
    execv(container_args[0], container_args);
    printf("Something's wrong!\n");
    return 1;
}
```

![mount_namespace_show_demo_new](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/mount_namespace_show_demo_new.png)

现在就会发现，对文件的隔离已经生效了，并且在子进程内部新创建的目录宿主机中一样看不到了，因为子进程的`/tmp`已经被挂载进了`tmpfs（一个内存盘）`中了，这就相当于创建了完全一个新的`tmp`环境。

为什么要花这么大的篇幅来说这个呢，是因为它就`是Docker镜像的主要技术之一`。上面的栗子，我只是把`tmp`这个目录进行了挂载，同样的，在Cloud Foundry中挂载隔离的文件也都是应用所需要的一些目录，而Docker镜像做的，是把`整个根目录`全部挂载进了这个新的进程中。

> 这里要稍微介绍一下Linux系统地文件结构的知识：和我们平时使用的Windows操作系统不同，Linux操作系统的系统**内核**与**操作系统所包含的文件、配置和目录**是`分开存放的`，系统只有在开机启动时才会加载指定版本的内核镜像。Linux所使用到的文件配置目录被称为`根文件系统（rootfs）`，也就是我们日常操作Linux看到的`/`目录。

Docker镜像的本质，其实也就是对rootfs的一次封装，Docker将一个应用所需操作系统的rootfs，通过`Mount Namespace`进行了封装，这样带来的直接结果就是：**改变了应用程序和操作系统的依赖关系**（这个很类似于强哥上一次讲到的控制反转的理念）即原本应用程序是在操作系统内运行的，而Docker把“操作系统”封装变成了应用程序的依赖库，这样就解决了应用程序运行环境`一致性`的问题，因为不管你是在自己主机上，还是服务器上，又或者是其他什么机器上，因为应用所运行的系统已经成了一个“依赖库”了，所以每个地方运行，这个一致性是完全可以保证的。

但是目前只解决了一致性，复用性的问题还没有解决，即我不可能每制作一个镜像就挂载一个新的rootfs吧，这样先不说复用性了，光硬盘都需要占用很大的空间来实现这些挂载内容。因此，Docker还为镜像使用了`另一个主要技术：UnionFS`以及一个`全新的概念：层（layer）`。

先说一下`UnionFS`是干什么的，UnionFS是一个联合挂载的功能，它可以将多个路径下的文件联合挂载到同一个目录下，举个栗子，现在有一个如下的目录结构：

``` shell
$ tree
.
├── A
│   ├── a
│   └── x
│ 
└── B
    ├── b
    └── x
```

A目录下有a和x两个文件，B目录下有b和x两个文件，通过UnionFS的功能，我们可以将这两个目录挂载到C目录下：

``` shell
$ tree ./C
C
├── a
├── b
└── x
```

最终C目录下的x只有一份，并且如果你对C目录下的a、b、x修改，之前目录A和B中的文件同样会被修改。而Docker正是用了这个技术，对其镜像进行了联合挂载，比如可以分别把`/sys`，`/etc`,`/tmp`目录一起挂载到rootfs中形成一个在子进程看起来就是一个完整的rootfs。

此外，Docker还自己创新了一个层的概念，它将系统内核所需要的rootfs内的文件挂载到了一个`只读层`中，将用户的应用程序、系统的配置文件等之类可以修改的文件挂载到了`可读写层`中，并且在容器启动时可能会用到的初始化参数挂载到了`init层`中。之后，将这三层再次联合挂载，最终形成了容器中的rootfs。这时你应该明白了，这个`只读层`是几乎同样版本的代码几乎不会改变的，比如我们的.net core项目用到的基础环境等等都会设计在了只读层，每次获取最新镜像时，因为每一份只读层都是完全一样的，所以完全不用下载，这也解释了为什么Docker镜像只在第一次下载时那么慢，而之后的镜像都很快，并且明明每份镜像看起来都几百兆，但是最终机器上的硬盘缺没有占用那么多的原因。也是这个技术，解决了镜像复用性的问题。我们可以看一下Docker镜像分层的图：

![docker_layer](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/docker_layer.png)

这里有个问题不知道你有没有考虑过，如果我们的程序做的是删除只读层中的文件，那会发生什么事呢？其实解决方案是：Docker在可读写层加入了一个whiteout的文件夹（BLM政治不正确），所以在只读层做的操作其实就在这个文件夹中加入了一个新的文件，然后在容器中就通过这个文件做了`“逻辑删除”`，只是做了`“遮挡”`的操作，原本的文件还是存在的。

好了，以上就是Docker容器的整个原理了，我们可以总结一下，Docker创建容器的过程其实是：

1. 启用Linux Namespace配置；
2. 设置指定的Cgroup参数；
3. 切换进程的根目录

对于制作一个Docker镜像，Docker使用了联合挂载的功能，和创建了层的概念，解决了镜像的一致性和复用性。

### 对比与思考

其实Docker还做了很多功能，比如权限配置，DeviceMapper等等，这里说的仅仅是一个普及性质的概念性讲解，底层的各种实现还有很复杂的概念。并且或多或少地，你肯定也会有一个想法，这个容器和传统的虚拟机有啥区别？

其实容器技术和虚拟机是实现虚拟化技术的两种手段，只不过虚拟机是通过Hypervisor控制硬解，模拟出一个GustOS来做虚拟化的，其内部是一个几乎真实的虚拟操作系统，内部外部是完全隔离的。而容器技术是通过Linux操作系统的手段，通过类似于Docker Engine这样的软件对系统资源进行的一次隔离和分配，它们之间的对比关系大概如下：

![compare_with_v_machine](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/compare_with_v_machine.jpg)

虚拟机相较于Docker容器来说更加安全，因为其是物理隔离的，但是这带来一个后果：一个运行着 CentOS 的 KVM 虚拟机启动后，在不做优化的情况下，虚拟机自己就需要占用 100~200 MB 内存。此外，用户应用运行在虚拟机里面，它对宿主机操作系统的调用就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘 I/O 的损耗非常大。相反：容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的；而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。

因此，更加符合现在“敏捷”和“高性能"的容器就成为了PaaS这种更需要细粒度资源管理的平台佼佼者。

但是容器的`弊端`也特别明显，因为容器是模拟出来的隔离性，所以对Namespace模拟不出来的资源：比如操作系统内核就完全无法隔离，`容器内部的程序和宿主机是共享操作系统内核的，也就是说，一个低版本的Linux宿主机很可能是无法运行高版本容器的`。还有一个典型的栗子就是`时间`，`如果容器中通过某种手段修改了系统时间，那么宿主机的时间一样会改变`。

`还有一个弊端就是安全性`，一般的企业，是不会直接把容器暴露给外部用户直接使用的，因为容器内可以直接操作内核代码，如果黑客可以通过某种手段修改内核程序，那就可以黑掉整个宿主机，这也是为什么我们组从刚开始自己写Docker到最后弃用的直接原因。现在一般解决安全性的方法有两个：一个是限制Docker内进程的运行权限，控制它值能操作我们想让它操作的系统设备，但是这需要大量的定制化代码，因为你可能并不知道它需要操作什么；另一个方式是在容器外部加一层虚拟机实现的沙箱，这也是阿里云以及美团的主要实现方式，想要具体了解，可以看这篇文档[云原生之容器安全实践](https://tech.meituan.com/2020/03/12/cloud-native-security.html)。

## 起于“鲸鱼”止于“舵手”

### 群雄纷争

让我们来继续讲故事。

上节说到，Docker项目依靠自己创新的Docker Image瞬间爆红。与此同时，很多厂商也从中发现了商机，纷纷推出自己的容器产品抢占市场，比如CoreOS推出了Rocket（rkt）容器，Google也开源了自己的容器项目lmctfy（Let Me Container That For You）等，但是面对Docker项目的强势，就算是Google也毫无招架之力，因此Google打算和Docker公司开展合作，关停自己的容器项目，并且和Docker公司一同维护开源的容器运行时，但是Docker公司很强势的拒绝了这个明显会削弱自己地位的合作。并且Docker公司此时也意识到了，自己仅仅是云计算技术栈中的幕后英雄，只能当做平台最终部署应用的载体，要想成为这个领域的核心，就应该有自己的生态。因此，在爆火并且有了充足的资金之后，Docker公司开始了疯狂的买买买来扩充自己的实力，其中最出名的就署名`Fig`，它是出名的`Docker Compose`项目的前身。通过这些并购与自身研发迭代，Docker公司最终推出了以自己为核心的PaaS三件套：`Docker Compose`、`Docker Swarm`以及`Docker Machine`，并且通过三件套迅速成为了当时行业内的霸主。此外，Docker公司还做出了一个更加惊人的动作：将公司名由dotCloud改名为Docker，并且将`Docker`注册成了自己的商标，开展自己的商业化路径。但是这样做同时也意味着，其他公司使用Docker就要给Docker公司支付大额的授权费用，这为Docker以后的没落埋下了伏笔。

这里稍微说一下Docker三件套：

1. Docker Compose：Compose的前身是一个仅有两个人维护的Docker容器编排的开源项目Fig，其功能是：假如现在用户需要部署一个应用A和一个数据库B以及负载均衡C，那么Fig就允许用户把ABC三个容器定义在一个配置文件中，并且指定它们的关联关系。最后只需要一条命令`fig up`即可直接使用。在Docker大火的时候，Fig项目在Github上的热度堪比Docker，因此在2015年Docker公司将其收购，并且改名为Docker Compose作为Docker容器的编排工具，并且使用至今
2. Docker Swarm：Swarm是Docker的集群管理项目，其主要的逻辑就和上一节讲过的Cloud Foundry类似，可以以类似于`docker run 我的镜像`的命令行方式，以`docker run -H 我的Swarm集群API地址 我的镜像`这样将Docker运行成一个集群并且进行管理，Swarm的出现将Docker项目从一个普通的容器升级成为PaaS平台
3. Docker Machine：Machine应该是Docker三件套里最不出名的了，我对此也没有过多了解，只知道它的功能是将虚拟机运行成容器，并且可以以管理Docker容器的方式管理虚拟机

然而人红是非多，更何况你还抢了别人的蛋糕，由于Docker公司在Docker开源项目的发展上始终保持着绝对的权威和发言权，并且在多个场合用实际行动挑战到了其他玩家，并且，Docker公司的商业化路径严重侵害了曾今的合作公司RedHat的切身利益，再加之，Docker在2015年间的高速迭代表中现出了各种不稳定的breaking change开始让社区叫苦不迭。于是，容器领域的其他几位玩家开始商讨“切割”Docker项目的话语权。最终，Docker公司迫于压力，于2015年6月22日，牵头了CoreOS、Google、RedHat等公司，共同成立了一个中立的基金会，并将自己的容器运行时LibContainer捐出，改名为RunC，基金会依据RunC共同制定了一套容器和镜像的标准和规范——OCI（Open Container Initiative），OCI的成立，意味着容器运行时和镜像的实现与Docker项目完全剥离，让其他玩家不依赖Docker实现自己的Docker运行时成为可能。

但是，由于成立基金会只是Docker公司对这些大公司的妥协，并且其当时确实维护着数量足够庞大的社区，因此Docker公司对基金会的成立有恃无恐，并且对标准的制定并不关心。因为在当时的大环境下，Docker自己的实现，已经就是业界统一的标准了。

### 天下归一

由于OCI基金会发展十分缓慢，因此Google、RedHat等基础设施领域的头部玩家打出了手中的王牌，共同牵头成了一个名叫`CNCF（Cloud Native Computing Foundation）的基金会`（这也是云原生这个概念第一次出现在大众视野内），并且基金会成立的思想基础是，以`Kubernetes项目`为基础，建立一个由开源基础设施领域厂商主导的按照独立基金会方式运营的平台级社区，来对抗以Docker为核心的容器商业生态。也正是这个基金会的成立，我们第二节的主人公`Kubernetes`也开始崭露头角了。

`Kubernetes`是Google公司早在2014年就发布开源的一个`容器基础设施编排`框架，和其他拍脑袋想出来的技术不同，`Kubernetes`的技术是有理论依据的，即：`Borg`。[Borg](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43438.pdf)是Google公司整个基础设施体系中的一部分，从已发布的博士论文中可以看到Brog是正是最核心的模块，其上承载了比如MapReduce、BigTable等诸多业界的头部技术。因此Borg系统一直以来都被誉为Google公司内部最强大的“秘密武器”，也是Google公司最不可能开源的项目，而`Kubernetes`就是在这样的理论基础上开发的。下图是Google Omega论文所描述的Google已公开的基础设施栈。

![borg_infr](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/borg_infr.png)

但是，也正由于开发灵感和设计思想来源于Borg，`Kubernetes`项目在刚发布时就被称为`曲高和寡`，太过超前的思想让开发者无法理解，并且也一直由Google的工程师自行维护，所以在发布之初并没有获得太多的关注和成长。然而，CNCF的成立改变了这一切，RedHat的长处就是有着成熟的社区管理体系，并且也有足够多的工程力，这使得`Kubernetes`项目在交由社区维护开始迅速发展。

那吹了这么半天，`Kubernetes`和`Docker Swarm`到底有什么区别呢？

我们知道，正常的工业级项目中，不可能存在单一应用可以独立运行的，大部分场景都是很多个应用共同配合组成一个庞大的集群系统。

Docker Swarm提供给用户功能就和之前Cloud Foundry类似，对用户的应用进行集群化的部署和管理，如果集群之间存在联系，则需要通过Docker Compose对应用之间建立一个`link`关系，这些关系信息主要就是应用之间的IP地址等，当一个应用的IP地址发生变化时，Docker Swarm负责处理和这个应用产生关系的其他应用的IP配置。因此，整个集群中就产生了无数个单一的`link`关联。

而与Docker Swarm不同，`Kubernetes`做的是比定义关系更高一层的事，它将应用于应用之间的关系进行了一次抽象，将所有应用的关系分成了`紧密交互`和`信息交换`两种，并且自定义了高于容器级别的`Pod`，用于更好的抽象这些概念（对于这些概念性的内容我会在之后的段落中给与总结和说明），如果说Docker Swarm提供给用户的是一个集群管理的软件的话，那Kubernetes就是管理集群编排的一个操作系统。并且，和Docker Swarm将Docker为核心不同，Kubernetes项目中对容器的依赖只是执行编排之后最终执行的成员而已，对Docker并不是紧密依赖的（比如上文提到过的CoreOS公司的rkt容器，就同样可以作为Kubernetes工程的部署容器）。此外，相比于Docker公司逐渐走向的封闭和垄断化，`Kubernetes`则反其道而行提倡开放式、民主式的架构，从API到容器运行时的每一层，Kubernetes项目都为开发者暴露出了可以扩展的插件机制，鼓励用户通过代码的方式介入Kubernetes项目的每一个阶段。

随着更切近为微服务架构的系统，以及更加民主和开放性的社区，2016年之后的容器社区已经完全成为以Kubernetes项目为核心的“百家争鸣”。而面对Kubernetes社区的崛起和壮大，Docker公司不得不承认自己的豪赌以失败告终，2017年开始，Docker将Docker项目的容器运行时部分Containerd捐赠给了CNCF社区，并且在10月宣布将在自己的Docker企业版中内置Kubernetes项目，这也标志着持续了近两年的容器编排之战落下帷幕。2018年1月，RedHat公司宣布斥资2.5亿美元收购CoreOS，2018年3月，这一切纷争的始作俑者Docker公司的CTO Solomon Hykes宣布辞职，至此，纷扰的容器技术圈尘埃落定，天下归一。

# 参考文档

* 极客时间：[深入剖析Kubernetes](https://time.geekbang.org/column/intro/100015201)、[Elasticsearch核心技术与实践](https://time.geekbang.org/course/intro/100030501)
* CoolShell：[Linux PID 1 和Systemd](https://coolshell.cn/articles/17998.html)、[DOCKER基础技术：LINUX NAMESPACE（上）](https://coolshell.cn/articles/17010.html)
* Wikipedia：[文件系统层次结构标准](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard)
* [Linux 线程实现机制分析](https://www.ibm.com/developerworks/cn/linux/kernel/l-thread/index.html)
* [YAML 语言教程](http://www.ruanyifeng.com/blog/2016/07/yaml.html)