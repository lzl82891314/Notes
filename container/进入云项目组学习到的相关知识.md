# 云项目组“上天”过程中发现的那些“神仙”

来云组已经两个月有余了，在我们组上天的过程中，碰到了很多之前开发中完全用不到的技术知识点，也就是我所说的“神仙”，今天我就来给大家盘一盘这些个“神仙”到底都是些什么玩意。

## 继续从PaaS说起

### 容器的兴起

强哥在之前的某次sprint技术分享会中给大家讲过了PaaS和SaaS等概念，那我这次就继续从PaaS开始，先来讲讲他的前世今生吧。

早在2013年前后，服务端的技术是云端计算的天下，当时有大量的云计算厂商涌入市场，比较著名的有`AWS`和`OpenStack`，他们将虚无缥缈的云端计算这种高大上的概念，固化成了实实在在的虚拟机技术，当时的主流用户，也都是将自己的应用上传到云端的虚拟机中，以之前管理物理机的方式管理着可能是一群云端的虚拟服务器。

而就在这个时候，一家主打开源PaaS项目的平台`Cloud Foundry`诞生了。与传统的提供云上虚拟机服务不同，Cloud Foundry的PaaS所提供的服务是`应用托管`的功能，也就是用户只需要通过一条简单的命令比如：`cf push "我的应用"`，就可以把自己的项目部署在可能成千上万台的终端服务器上。然后平台对这些项目提供分发，灾备，监控，重启等等服务（这其实也是我们组想要给用户提供的最终服务），这些托管服务解放了开发者的生产力，让他们不用再关心应用的运维状况，而是专心开发自己的应用。这就是PaaS的概念，平台即服务。

因此，像Cloud Foundry这样的PaaS项目，最核心的组件就是对一套应用的打包和分发机制，Cloud Foundry为每一种主流的语言都定义了一套打包的方式。而cf push命令，就是将应用打好的包上传到云端的存储中，然后Cloud Foundry调度出一个可运行的虚拟机，通知该虚拟机上的Agent下载并且运行这个打包结果。由此可见，Cloud Foundry的虚拟机中会运行多个用户的多个服务，为了解决服务之间相互隔离的边界问题，Cloud Foundry使用了Linux的Namespace技术对运行的应用进行了隔离和分组，使用Cgroup技术对隔离的应用分配资源，而这也正是容器技术的一部分。

由此我们可以知道，容器技术并不是Docker创建的，而且在Docker兴起之前，就已经被其他公司商用了，但是为什么现在一谈起容器，所有人第一时间想到的就是Docker呢？

上面，我提到了Cloud Foundry的部署需要用户对其应用进行打包，然而就是这个打包的功能，成了Cloud Foundry的一个软肋一直被用户诟病：为了上传PaaS，用户就不得不为每一种语言，每一种框架，甚至是每个版本应用维护一个打好的包，这种打包的方法是毫无章法的，还有可能出现本机运行成功，打了个包上传上去之后就无法运行的情况。然而就在这个时候，这一节的主人`Docker`开始崭露头角了。

Docker是一个当时还叫dotCloud的公司开发的容器项目，在开源的短短几个月后就迅速崛起，并一局将Cloud Foundry赶出了局，然而最可笑的是，在Docker刚开源的时候，Cloud Foundry的首席产品经理 James Bayer就在社区做了一次详细的对比，告诉用户Docker和Cloud Foundry一样是一个使用了Namespace和Cgroup技术的沙箱而已，没什么值得关注的。事实上，Docker也确实就和他所说的一样，但是只做了一点小小的创新，而就是这一点小小的创新，对Cloud Foundry造成了毁灭性的降维打击。这个创新就是`Docker镜像`。

Docker镜像几乎完美地解决了Cloud Foundry对于打包方面的软肋。所谓的镜像，其实也是一个压缩包，但是比起Cloud Foundry那种执行文件+启动脚本的打包结果，镜像提供给用户的是一套完整的运行环境，每一个镜像都可以指定操作系统版本，内部可以构建程序执行的文件结构，并且一份镜像可以完全共享多处使用。Docker给用户提供了一套完善的镜像制作流程，用户也就不用关心自己的语言和框架了，只需要定制对应程序所需要的运行的操作系统环境即可（这一点其实也可以简单的看成是对压缩包这种应用做了底层的抽象，比较契合软件设计的原则。也可以看到，软件行业的一点创新，带来的就是降维级别的打击）。总结一下就是：`Docker 镜像完美解决了两个问题：1是本地环境和服务器环境的差异问题；2是一份镜像所有的机器都可以运行的复用问题`。

在这之后，PaaS的市场，已经完全是Docker的天下了。而之后的故事，我会在下一节再继续讲。

### 一个进程的诞生

好了，讲了半天的故事了，也要说一点干货了：容器到底是个什么玩意，能有这么大的市场和这么多服务端程序员对其青睐？先卖个关子，我先从一个进程说起。

我们都知道，计算机里运行的程序其实都是一个一个的进程，而一个进程其实就是程序执行之后，从磁盘的二进制文件，到内存、寄存器、堆栈指令等等所用到的相关设备状态的一个集合，是`数据和状态综合的动态表现`。而容器技术，其实就是对一个进程的状态和数据进行的一系列`隔离`和`限制`后的结果。因此可以知道，`容器的本质其实就是Linux中的一个特殊进程`。

先来说说隔离，我们可以在任意一个装有Docker程序的Linux中执行下列的命令创建一个简单的镜像：

``` shell
$ docker run -it busybox /bin/sh
```

这条语句的大概意思是：用docker运行一个容器，容器的镜像名称叫`busybox`，并且运行之后需要执行的命令是`/bin/sh`，而`-it`参数表示需要使用标准输入`stdin`和分配一个文本输入输出环境`tty`与外部交互。通过这个命令，我们就可以进入到一个容器内部了，分别在容器中和宿主机中执行`top`命令，可以看到以下结果：

![host_container_comparer](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/host_container_comparer.png)

可以发现，容器中的运行进程只剩下了两个，一个就是主进程PID==1的/bin/sh，另一个就是我们运行的top，而宿主机中的其余的所有进程在容器中都看不到了，这就是`隔离`。本来，每当我们在宿主机上运行一个/bin/sh程序，操作系统都会给它分配一个进程编号，比如PID==100。而现在，我们要通过Docker把这个/bin/sh程序运行在一个容器中，这时候，Docker就会在这个PID==100创建时施加一个“障眼法”，让他永远看不到之前的99个进程，这样运行在容器中的程序就会当自己是PID==1的主进程。

而这种机制，其实就是对被隔离的程序的进程空间做了手脚，虽然在容器中显示的PID==1，但是在原本的宿主机中，它其实还是那个PID==100的进程。所使用到的技术就是Linux中的`Namespace机制`。而这个机制，其实就是Linux在创建进程时的一个可选参数。在Linux中，创建一个线程的函数是（这里没写错就是线程，Linux中线程是用进程实现的，所以可以用来描述进程）：

``` shell
int pid = clone(main_function, stack_size, SIGCHLD, NULL);
```

如果我们给这个方法添加一个参数比如`CLONE_NEWPID`：

``` shell
int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL);
```

那么这个新的进程就会看到一个全新的进程空间，在这个空间里，它自己的PID就等于1。

`除了刚才说的PID Namespace，Linux操作系统还提供了Mount、UTS、IPC、Network和User的Namespace，用来对各种不同的进程上下文进行隔离操作`。

以上，也就是Linux容器最基本的隔离的实现了。

说完了隔离，再来说说限制。容器的隔离性是通过Linux的`Cgroup`实现的。`Cgroup`的全称是`Linux Control Group`，是Linux操作系统中用来`限制一个进程组使用资源的上限，包括CPU、内存、磁盘、网络带宽等`的功能。在Linux中，Cgroup给用户暴露的API是文件系统，因此用户可以通过修改文件的值来操作Cgroup功能。在Linux系统（Ubuntu）中可以执行以下命令查看CgroupAPI文件：

``` shell
mount -t cgroup
```

![Cgroup_fs](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/Cgroup_fs.png)

从上图可以看到，系统中存在包括cpu、内存、IO等多个Cgroup配置文件。我们可以以CPU为例来说明以下Cgroup这个功能。对CPU的限制需要引入两个参数`cfs_period`和`cfs_quota`，这个强哥肯定知道当时为了给Docker内的程序限制CPU会经常操作这两个参数，这两个参数是组合使用的，意思是在长度为cfs_period时间内，程序组只能分到总量为cfs_quota的CPU时间。也就是说`cfs_quota / cfs_period == cpu使用上限`。

举个栗子，在`/sys/fs/cgroup/cpu`目录下，执行以下命令创建一个文件夹container：

``` shell
/sys/fs/cgroup/cpu/ > mkdir container
```

可以发现系统会自动为container目录下生成一系列的CPU限制的参数文件，这是Linux系统自动生成的，表示我们成功为CPU创建了一个控制组container：

![cpu_limit_file](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_limit_file.png)

然后我们可以执行一下以下脚本创建一个死循环程序：

``` shell
while : ; do : ; done &
```

会看到返回的`进程为398`，因为死循环所以top可以看到cpu占用率为100%：

![cpu_full](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_full.png)

这时，我们可以看下container目录下的`cpu.cfs_quota_us`和`cpu.cfs_period_us`：

![cpu_usage_file](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_usage_file.png)

`cfs_quota_us`为-1说明并没有限制CPU的运行上限，现在我们可以改一下这个值：

``` shell
echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
```

然后将之前的`进程398`写入这个控制组的`tasks`文件中：

``` shell
echo 398 > /sys/fs/cgroup/cpu/container/tasks
```

这时，我们可以再top一下，发现刚才的死循环的CPU使用率只有20%了：

![cpu_usage_after](https://raw.githubusercontent.com/lzl82891314/Notes/main/container/resource/cpu_usage_after.png)

以上，就是通过Cgroup功能对容器做限制的原理了，同理可以用此方法，对一个容器的内存、带宽等做限制，这样，一个简单的容器基本就可以展现在你面前了。


> 这里要做一个特别的说明，只有Linux中运行的容器是通过对进程进行限制模拟出来的结果，Windows和Mac下的容器，都是通过Docker Desktop操作虚拟机模拟出来的真实的虚拟容器。